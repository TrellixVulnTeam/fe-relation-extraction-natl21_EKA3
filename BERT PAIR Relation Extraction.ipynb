{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BERT PAIR Relation Extraction.ipynb","provenance":[{"file_id":"1S-npM9Umn_s4D3FS6Fyl_pEo5fopcuob","timestamp":1591960614367},{"file_id":"1_i6d5uaAiqpuUwvf5NiHxcXHf58kAqt1","timestamp":1591254255357}],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm","mount_file_id":"1H0MsHAw6tTnoH6SUzELHAdoXHxVD0vkV","authorship_tag":"ABX9TyMo8XTsfKLEzXTJjW54y5wo"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"jSZcpHBesDNw"},"source":["# **BERT PAIR Relation Extraction Notebook**\n"]},{"cell_type":"markdown","metadata":{"id":"ndloW5ceTTDV"},"source":["## Imports and environment configuration"]},{"cell_type":"code","metadata":{"id":"vWyMo4eeshmP"},"source":["!pip install transformers==3.0.0\n","!pip install ipython-autotime\n","\n","%load_ext autotime"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1nDTaiQhzRbO"},"source":["import os\n","import sys\n","import json\n","import random\n","from pathlib import Path\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","from transformers import BertTokenizer, BertForSequenceClassification\n","\n","if 'google.colab' in str(get_ipython()):\n","  print('Running on Google Colab')\n","  root = '/content/drive/My Drive/Colab Notebooks/'\n","else:\n","  print('Running locally')\n","  root = Path(os.getcwd()).parent\n","\n","basepath = os.path.join(root, 'relation-extraction/')\n","sys.path.append(os.path.join(basepath, 'bert-pair/code'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xUunXUqqSTNE"},"source":["from pair import Pair\n","from fewshot_re_kit.data_loader import FewRelDatasetPair\n","from fewshot_re_kit.framework import FewShotREFramework"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RLpt4eIeEb4C"},"source":["Switch for data usage: If True FewRel data will be used, if False Future Engineering data is used"]},{"cell_type":"code","metadata":{"id":"Uny2n32SEYDv"},"source":["use_fewrel_data=False"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TNvZPJbUgSSO"},"source":["Defining relevant_relations and paths to data files"]},{"cell_type":"code","metadata":{"id":"CyGIiDcqB1iy"},"source":["if (use_fewrel_data):\n","    relevant_relations = ['P105', 'P135', 'P155', 'P31', 'P800', 'P921']\n","\n","    data_dir = os.path.join(root, 'fewrel-training-data/bert-pair/')\n","\n","    train_file = os.path.join(data_dir, 'train_wiki')\n","    val_file = os.path.join(data_dir, 'val_wiki')\n","    test_file = os.path.join(data_dir, 'val_pubmed')\n","\n","    support_set_file_name = os.path.join(basepath, 'bert-pair/support_sets/support_fewrel_%d_%d.json' %(len(relevant_relations), 3))\n","else:\n","    relevant_relations = ['A manufactures product B', 'A operates B', 'A operates \\[something\\] in location B', 'A orders B', 'A uses/employs charging technology B', 'A orders something from B']\n","    \n","    data_dir = os.path.join(root, 'fe-training-data/')\n","\n","    train_file = os.path.join(data_dir, 'train_examples_nota_manufact_operate_operatesth_order_uses_ordersth_per_label')\n","    val_file = os.path.join(data_dir, 'test_examples_nota_manufact_operate_operatesth_order_uses_ordersth_per_label')\n","    test_file = os.path.join(data_dir, 'test_examples_nota_manufact_operate_operatesth_order_uses_ordersth_per_label')\n","\n","    support_set_file_name = os.path.join(basepath, 'bert-pair/support_sets/support_fe_%d_%d.json' %(len(relevant_relations), 3))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EQWk8iPigdfv"},"source":["Sentence encoder class for the BERT Pair approach which manages the model and the tokenizer"]},{"cell_type":"code","metadata":{"id":"0U4gpuIFRpjP"},"source":["class BERTPAIRSentenceEncoder(nn.Module):\n","    def __init__(self, pretrain_path, max_length): \n","        nn.Module.__init__(self)\n","        self.bert = BertForSequenceClassification.from_pretrained(\n","                pretrain_path,\n","                num_labels=2)\n","        self.max_length = max_length\n","        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","    def forward(self, inputs):\n","        x = self.bert(inputs['word'], token_type_ids=inputs['seg'], attention_mask=inputs['mask'])[0]\n","        return x\n","    \n","    def tokenize(self, raw_tokens, pos_head, pos_tail):\n","        # token -> index\n","        # tokens = ['[CLS]']\n","        tokens = []\n","        cur_pos = 0\n","        pos1_in_index = 0\n","        pos2_in_index = 0\n","        for token in raw_tokens:\n","            token = token.lower()\n","            if cur_pos == pos_head[0]:\n","                tokens.append('[unused0]')\n","                pos1_in_index = len(tokens)\n","            if cur_pos == pos_tail[0]:\n","                tokens.append('[unused1]')\n","                pos2_in_index = len(tokens)\n","            tokens += self.tokenizer.tokenize(token)\n","            if cur_pos == pos_head[-1]:\n","                tokens.append('[unused2]')\n","            if cur_pos == pos_tail[-1]:\n","                tokens.append('[unused3]')\n","            cur_pos += 1\n","        indexed_tokens = self.tokenizer.convert_tokens_to_ids(tokens)\n","        \n","        return indexed_tokens"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RAWjmvU7QvaC"},"source":["## Fine-Tuning"]},{"cell_type":"markdown","metadata":{"id":"2nxFWLQffoLB"},"source":["Defining some parameters for training of the model"]},{"cell_type":"code","metadata":{"id":"Nh7ojhQcQ49C"},"source":["trainN = 6\n","N = 6\n","K = 1\n","Q = 1\n","batch_size = 2\n","max_length = 100\n","hidden_size = 768\n","na_rate = 5\n","\n","val_step = 1000\n","train_iter = 10000\n","val_iter = 1000\n","test_iter = 1000\n","\n","if (use_fewrel_data):\n","    ckpt = os.path.join(basepath, 'bert-pair/checkpoint/bert-pair-fewrel-N6-K1.pth.tar')\n","    prefix = 'bert-pair-fewrel-N6-K1.pth.tar'\n","else:\n","    ckpt = os.path.join(basepath, 'bert-pair/checkpoint/bert-pair-fe-N6-K1.pth.tar')\n","    prefix = 'bert-pair-fe-N6-K1.pth.tar'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NWYM_oUYfzAz"},"source":["Initializing sentence encoder and model for BERT Pair"]},{"cell_type":"code","metadata":{"id":"jRN6DIdIRWrk"},"source":["sentence_encoder = BERTPAIRSentenceEncoder('bert-base-uncased', max_length)\n","\n","model = Pair(sentence_encoder, hidden_size=hidden_size)\n","\n","if torch.cuda.is_available():\n","    model.cuda()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"seYo0E1Hf8vo"},"source":["Loading train-, validation- and test-data and initializing the FewShotREFramework with the different data loaders"]},{"cell_type":"code","metadata":{"id":"s83boei_LbND"},"source":["def collate_fn_pair(data):\n","    batch_set = {'word': [], 'seg': [], 'mask': []}\n","    batch_label = []\n","    fusion_sets, query_labels = zip(*data)\n","    for i in range(len(fusion_sets)):\n","        for k in fusion_sets[i]:\n","            batch_set[k] += fusion_sets[i][k]\n","        batch_label += query_labels[i]\n","    for k in batch_set:\n","        batch_set[k] = torch.stack(batch_set[k], 0)\n","    batch_label = torch.tensor(batch_label)\n","    return batch_set, batch_label\n","\n","def get_loader_pair(name, encoder, N, K, Q, batch_size, \n","        num_workers=0, collate_fn=collate_fn_pair, na_rate=0, root='./data', encoder_name='bert'):\n","    dataset = FewRelDatasetPair(name, encoder, N, K, Q, na_rate, root, encoder_name)\n","    data_loader = torch.utils.data.DataLoader(dataset=dataset,\n","            batch_size=batch_size,\n","            shuffle=False,\n","            pin_memory=True,\n","            num_workers=num_workers,\n","            collate_fn=collate_fn)\n","    return iter(data_loader)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aG4Ht8KJYsvb"},"source":["train_data_loader = get_loader_pair(train_file, sentence_encoder, N=trainN, K=K, Q=Q, na_rate=na_rate, batch_size=batch_size, encoder_name='bert')\n","val_data_loader = get_loader_pair(val_file, sentence_encoder, N=N, K=K, Q=Q, na_rate=na_rate, batch_size=batch_size, encoder_name='bert')\n","test_data_loader = get_loader_pair(test_file, sentence_encoder, N=N, K=K, Q=Q, na_rate=na_rate, batch_size=batch_size, encoder_name='bert')\n","\n","framework = FewShotREFramework(train_data_loader, val_data_loader, test_data_loader)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XJhP6ew3fe1-"},"source":["Training the model using the provided FewShotREFramework from the authors of the FewRel dataset"]},{"cell_type":"code","metadata":{"id":"foOVYGzHUuf9"},"source":["framework.train(model, prefix, batch_size, trainN, N, K, Q,\n","        pytorch_optim=optim.SGD, na_rate=na_rate, val_step=val_step, pair=True, \n","        train_iter=train_iter, val_iter=val_iter, bert_optim=True,\n","        save_ckpt=ckpt)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5Lx_q_xTmGrp"},"source":["## Generation of support set for BERT Pair approach"]},{"cell_type":"code","metadata":{"id":"EQC1KtCXmbdp"},"source":["K = 3\n","\n","json_data_train = json.load(open(train_file + \".json\"))\n","json_data_val = json.load(open(val_file + \".json\"))\n","json_data = {**json_data_train, **json_data_val}\n","\n","labels_support = []\n","support = []\n","\n","# building support set out of random example sentences from the dataset\n","for i, class_name in enumerate(relevant_relations):\n","    indices = np.random.choice(list(range(len(json_data[class_name]))), K, False)\n","\n","    for j in indices:\n","        item = json_data[class_name][j]\n","        word = sentence_encoder.tokenize(item['tokens'], item['h'][2][0], item['t'][2][0])\n","        support.append(word)\n","\n","    labels_support.append((i, class_name))\n","\n","with open(support_set_file_name, 'w') as support_file:\n","    support_obj = dict()\n","    support_obj['labels_support'] = labels_support\n","    support_obj['support_set'] = support\n","    json.dump(support_obj, support_file)"],"execution_count":null,"outputs":[]}]}