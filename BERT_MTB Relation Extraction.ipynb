{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BERT/MTB Relation Extraction.ipynb","provenance":[{"file_id":"1_i6d5uaAiqpuUwvf5NiHxcXHf58kAqt1","timestamp":1591254255357}],"collapsed_sections":[],"machine_shape":"hm","mount_file_id":"1S-npM9Umn_s4D3FS6Fyl_pEo5fopcuob","authorship_tag":"ABX9TyNjZtq8/Q5WKiJDStN6chPM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"jSZcpHBesDNw"},"source":["# **BERT Relation Extraction Notebook**\n"]},{"cell_type":"markdown","metadata":{"id":"ndloW5ceTTDV"},"source":["## Imports and environment configuration"]},{"cell_type":"code","metadata":{"id":"vWyMo4eeshmP"},"source":["!pip install seqeval\n","!pip install boto3\n","!pip install transformers==3.0.0\n","!pip install ipython-autotime\n","\n","%load_ext autotime"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1nDTaiQhzRbO"},"source":["import os\n","import sys\n","import math\n","import time\n","import json\n","import random\n","import pandas as pd\n","from pathlib import Path\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","from sklearn.metrics import f1_score, precision_score, recall_score\n","\n","random.seed(42)\n","\n","if 'google.colab' in str(get_ipython()):\n","  print('Running on Google Colab')\n","  root = '/content/drive/My Drive/Colab Notebooks/'\n","else:\n","  print('Running locally')\n","  root = Path(os.getcwd()).parent\n","\n","basepath = os.path.join(root, 'relation-extraction/')\n","sys.path.append(os.path.join(basepath, 'MTB/code'))\n","\n","from modeling_bert import BertModel as Model\n","from tokenization_bert import BertTokenizer as Tokenizer"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RLpt4eIeEb4C"},"source":["Switch for data usage: If True FewRel data will be used, if False Future Engineering data is used"]},{"cell_type":"code","metadata":{"id":"Uny2n32SEYDv"},"source":["use_fewrel_data=False"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BVif7LRPYTMA"},"source":["## Matching the Blanks Pre-Training"]},{"cell_type":"markdown","metadata":{"id":"3O7v6LOdswN2"},"source":["The pre-training process of Matching the Blanks can run for multiple days, even with GPU support. Therefore a already pre-trained model is provided in the GitLab repository. For additional information see README."]},{"cell_type":"code","metadata":{"id":"jDxU7g7jSdPw"},"source":["!pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.3.1/en_core_web_lg-2.3.1.tar.gz\n","\n","import en_core_web_lg"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"endwEDwhOO5V"},"source":["### Pre-Training Helper functions"]},{"cell_type":"code","metadata":{"id":"dGsaEI9aJekl"},"source":["from pretrain_helper_functions import Two_Headed_Loss, pretrain_dataset, Mtb_Pad_Sequence\n","from pretrain_helper_functions import load_state, get_subject_objects, create_pretraining_corpus, process_textlines, mtb_evaluate_"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pyrrR8TR7Q49"},"source":["def mtb_load_dataloaders(pretrain_data, batch_size, max_length=50000):\n","    print(\"Loading pre-training data...\")\n","    with open(pretrain_data, \"r\", encoding=\"utf8\") as f:\n","        text = f.readlines()\n","    \n","    text = process_textlines(text)\n","    \n","    print(\"Length of text (characters): %d\" % len(text))\n","    num_chunks = math.ceil(len(text)/max_length)\n","    print(\"Splitting into %d max length chunks of size %d\" % (num_chunks, max_length))\n","    text_chunks = (text[i*max_length:(i*max_length + max_length)] for i in range(num_chunks))\n","    \n","    D = []\n","    print(\"Loading Spacy NLP...\")\n","    nlp = en_core_web_lg.load()\n","    \n","    for text_chunk in text_chunks:\n","        D.extend(create_pretraining_corpus(text_chunk, nlp, window_size=40))\n","        \n","    print(\"Total number of relation statements in pre-training corpus: %d\" % len(D))\n","   \n","    train_set = pretrain_dataset(D, tokenizer, batch_size=batch_size)\n","    train_length = len(train_set)\n","\n","    return train_set"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mWfibeaP5pFG"},"source":["### Pre-Training with Matching the Blanks"]},{"cell_type":"markdown","metadata":{"id":"-wtQbr9UvSkS"},"source":["Definition of parameters for pre-training with Matching the Blanks"]},{"cell_type":"code","metadata":{"id":"esNjgI1ZJqRF"},"source":["num_epochs=18\n","freeze=0\n","lr=0.0001\n","max_norm=1.0\n","gradient_acc_steps=2\n","batch_size=32\n","pretrain_data=os.path.join(root, 'fewrel-training-data/MTB/cnn.txt')\n","checkpoint_path = os.path.join(basepath, 'MTB/pretrain_checkpoints/pretrain_checkpoint_BERT_1.pth.tar')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NyZTADPcvez9"},"source":["Loading model and tokenizer and initialize optimizer and scheduler for training routine."]},{"cell_type":"code","metadata":{"id":"vZZ3HVZ6PW3U"},"source":["model_name = 'bert-base-uncased'\n","lower_case=True\n","\n","tokenizer = Tokenizer.from_pretrained(model_name, do_lower_case=lower_case)\n","tokenizer.add_tokens(['[E1]', '[/E1]', '[E2]', '[/E2]', '[BLANK]'])\n","\n","mtb_model = Model.from_pretrained(model_name, force_download=False)\n","mtb_model.resize_token_embeddings(len(tokenizer)) \n","\n","if cuda:\n","    print(\"Cuda is on\")\n","    mtb_model.cuda()\n","\n","if freeze == 1:\n","    print(\"FREEZING MOST HIDDEN LAYERS...\")\n","    unfrozen_layers = [\"classifier\", \"pooler\", \"encoder.layer.11\", \"encoder.layer.10\",\\\n","                        \"encoder.layer.9\", \"blanks_linear\", \"lm_linear\", \"cls\"]\n","        \n","    for name, param in mtb_model.named_parameters():\n","        if not any([layer in name for layer in unfrozen_layers]):\n","            print(\"[FROZE]: %s\" % name)\n","            param.requires_grad = False\n","        else:\n","            print(\"[FREE]: %s\" % name)\n","            param.requires_grad = True\n","    \n","criterion = Two_Headed_Loss(lm_ignore_idx=tokenizer.pad_token_id, use_logits=True, normalize=False)\n","optimizer = optim.Adam([{\"params\":mtb_model.parameters(), \"lr\": lr}])\n","\n","scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[2,4,6,8,12,15,18,20,22,24,26,30], gamma=0.8)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xIbxyzP2vYPE"},"source":["Loading pre-training data from inputfile."]},{"cell_type":"code","metadata":{"id":"yWTGkKZwPHLO","colab":{"base_uri":"https://localhost:8080/","height":119},"executionInfo":{"status":"ok","timestamp":1588406204969,"user_tz":-120,"elapsed":56519,"user":{"displayName":"Christoph Brandl","photoUrl":"","userId":"16180149632937868484"}},"outputId":"3c3dcc32-fcc6-404f-8bf7-6ee8926eb7dc"},"source":["cuda = torch.cuda.is_available()\n","\n","train_loader = mtb_load_dataloaders(pretrain_data, batch_size)\n","train_len = len(train_loader)\n","print(\"Loaded %d pre-training samples.\" % train_len)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Loading pre-training data...\n","Length of text (characters): 1041308\n","Splitting into 21 max length chunks of size 50000\n","Loading Spacy NLP...\n","Total number of relation statements in pre-training corpus: 14835\n","Loaded 14835 pre-training samples.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5Mca8BmavtbW"},"source":["Load checkpoint if available to continue training from this point."]},{"cell_type":"code","metadata":{"id":"9l0sSUi6PrpR","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1588406238406,"user_tz":-120,"elapsed":19716,"user":{"displayName":"Christoph Brandl","photoUrl":"","userId":"16180149632937868484"}},"outputId":"4a16a035-2dc7-4bd5-af39-470db724f3c1"},"source":["start_epoch, best_pred = load_state(mtb_model, optimizer, scheduler, checkpoint_path)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Loaded checkpoint model.\n","Loaded model and optimizer.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"RJZAxtxBv2r0"},"source":["Pre-Training process with Matching the Blank method. Caution: This kind of training runs at least 10 to 12 hours on limited hardware."]},{"cell_type":"code","metadata":{"id":"LZiCVBVjBmI4"},"source":["losses_per_epoch=[]\n","accuracy_per_epoch=[]\n","#start_epoch=0\n","\n","\n","print(\"Starting training process...\")\n","pad_id = tokenizer.pad_token_id\n","mask_id = tokenizer.mask_token_id\n","update_size = len(train_loader)//10\n","for epoch in range(start_epoch, num_epochs):\n","    start_time = time.time()\n","    mtb_model.train(); total_loss = 0.0; losses_per_batch = []; total_acc = 0.0; lm_accuracy_per_batch = []\n","    for i, data in enumerate(train_loader, 0):\n","        x, masked_for_pred, e1_e2_start, _, blank_labels, _,_,_,_,_ = data\n","        masked_for_pred1 =  masked_for_pred\n","        masked_for_pred = masked_for_pred[(masked_for_pred != pad_id)]\n","        if masked_for_pred.shape[0] == 0:\n","            print('Empty dataset, skipping...')\n","            continue\n","        attention_mask = (x != pad_id).float()\n","        token_type_ids = torch.zeros((x.shape[0], x.shape[1])).long()\n","\n","        if cuda:\n","            x = x.cuda(); masked_for_pred = masked_for_pred.cuda()\n","            attention_mask = attention_mask.cuda()\n","            token_type_ids = token_type_ids.cuda()\n","        \n","        blanks_logits, lm_logits, _ = mtb_model(x, token_type_ids=token_type_ids, attention_mask=attention_mask, Q=None,\\\n","                      e1_e2_start=e1_e2_start)\n","        lm_logits = lm_logits[(x == mask_id)]\n","        \n","        #return lm_logits, blanks_logits, x, e1_e2_start, masked_for_pred, masked_for_pred1, blank_labels, tokenizer # for debugging now\n","        if (i % update_size) == (update_size - 1):\n","            verbose = True\n","        else:\n","            verbose = False\n","            \n","        loss = criterion(lm_logits, blanks_logits, masked_for_pred, blank_labels, verbose=verbose)\n","        loss = loss/gradient_acc_steps\n","\n","        loss.backward()\n","\n","        grad_norm = nn.utils.clip_grad_norm_(mtb_model.parameters(), max_norm)\n","        \n","        if (i % gradient_acc_steps) == 0:\n","            optimizer.step()\n","            optimizer.zero_grad()\n","        \n","        total_loss += loss.item()\n","        total_acc += mtb_evaluate_(lm_logits, blanks_logits, masked_for_pred, blank_labels, \\\n","                                tokenizer, print_=False)[0]\n","        \n","        if (i % update_size) == (update_size - 1):\n","            losses_per_batch.append(gradient_acc_steps*total_loss/update_size)\n","            lm_accuracy_per_batch.append(total_acc/update_size)\n","            print('[Epoch: %d, %5d/ %d points] total loss, lm accuracy per batch: %.3f, %.3f' %\n","                  (epoch + 1, (i + 1), train_len, losses_per_batch[-1], lm_accuracy_per_batch[-1]))\n","            total_loss = 0.0; total_acc = 0.0\n","            print(\"Last batch samples (pos, neg): %d, %d\" % ((blank_labels.squeeze() == 1).sum().item(),\\\n","                                                                (blank_labels.squeeze() == 0).sum().item()))\n","    \n","    scheduler.step()\n","    losses_per_epoch.append(sum(losses_per_batch)/len(losses_per_batch))\n","    accuracy_per_epoch.append(sum(lm_accuracy_per_batch)/len(lm_accuracy_per_batch))\n","    print(\"Epoch finished, took %.2f seconds.\" % (time.time() - start_time))\n","    print(\"Losses at Epoch %d: %.7f\" % (epoch + 1, losses_per_epoch[-1]))\n","    print(\"Accuracy at Epoch %d: %.7f\" % (epoch + 1, accuracy_per_epoch[-1]))\n","    \n","    torch.save({\n","            'epoch': epoch + 1,\\\n","            'state_dict': mtb_model.state_dict(),\\\n","            'best_acc': accuracy_per_epoch[-1],\\\n","            'optimizer' : optimizer.state_dict(),\\\n","            'scheduler' : scheduler.state_dict(),\\\n","            'amp': None\n","        }, os.path.join(basepath, \"MTB/pretrain_checkpoints/pretrain_checkpoint_BERT_1.pth.tar\"))\n","\n","print(\"Finished Training!\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_ggrlVfjlEFV"},"source":["## Fine Tuning of classification approach with pre-trained MTB-model"]},{"cell_type":"markdown","metadata":{"id":"5SkBFHmCPeOb"},"source":["### Helper functions and commonly needed elements"]},{"cell_type":"code","metadata":{"id":"Ud39vhNyYKoz"},"source":["from fine_tuning_helper_functions import evaluate_"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ls8zKx7f5j8M"},"source":["Definition of parameters for fine-tuning"]},{"cell_type":"code","metadata":{"id":"-U5t8zo9lRXt"},"source":["model_name = 'bert-base-uncased'\n","lower_case=True\n","num_classes = 7\n","use_pretrained_blanks = 1\n","max_seq_length = 100\n","\n","gradient_acc_steps = 1\n","\n","num_epochs = 7\n","lr = 0.00005\n","batch_size = 32\n","\n","eval_batch_size = 8\n","\n","checkpoint_path = os.path.join(basepath, 'MTB/pretrain_checkpoints/pretrain_checkpoint_BERT_1.pth.tar')\n","\n","if (use_fewrel_data):\n","    data_dir = os.path.join(root, 'fewrel-training-data/fewrel/')\n","    test_data_file = \"test_%d_classes_disjoint.json\" % num_classes\n","    train_data_file = \"dev_%d_classes_disjoint.json\" % num_classes\n","\n","    data_type = 'fewrel'\n","else:\n","    data_dir = os.path.join(root, 'fe-training-data')\n","    test_data_file = 'test_examples_nota_manufact_operate_operatesth_order_uses_ordersth.json'\n","    train_data_file = 'train_examples_nota_manufact_operate_operatesth_order_uses_ordersth.json'\n","\n","    data_type = 'fe'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aESdCjLY5b92"},"source":["Initialization of model and tokenizer"]},{"cell_type":"code","metadata":{"id":"gembsmS5l32J"},"source":["from transformers import BertForSequenceClassification, BertTokenizer\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","model_name = 'bert-base-uncased'\n","\n","tokenizer = BertTokenizer.from_pretrained(model_name, do_lower_case=False)\n","tokenizer.add_tokens(['[E1]', '[/E1]', '[E2]', '[/E2]', '[BLANK]'])\n","\n","model = BertForSequenceClassification.from_pretrained(model_name, num_labels = num_classes)\n","model.resize_token_embeddings(len(tokenizer))\n","\n","if torch.cuda.is_available():\n","    model.cuda()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f92eAQ5G53jj"},"source":["FewrelProcessor class manages loading of training and test data"]},{"cell_type":"code","metadata":{"id":"93Cx0AGkpcJV"},"source":["class FewrelProcessor():\n","    def get_train_examples(self, data_dir,file_name):\n","        examples = self._create_examples(\n","            self._read_json(os.path.join(data_dir, file_name)), \"train\")\n","        labels = set([x[3] for x in examples])\n","        return examples, list(labels)\n","\n","    def get_dev_examples(self, data_dir,file_name):\n","        examples = self._create_examples(\n","            self._read_json(os.path.join(data_dir, file_name)), \"dev\")\n","        labels = set([x[3] for x in examples])\n","        return examples, list(labels)\n","        \n","    def get_test_examples(self, data_dir, file_name):\n","        examples = self._create_examples(\n","            self._read_json(os.path.join(data_dir, file_name)), \"test\")\n","        labels = set([x[3] for x in examples])\n","        return examples, list(labels)\n","\n","    def _create_examples(self, lines, set_type):\n","        \"\"\"Creates examples for the training and dev sets.\"\"\"\n","        examples = []\n","        for (i, line) in enumerate(lines):\n","            guid = \"%s-%s\" % (set_type, i)\n","            for x in line['ents']:\n","                if x[1] == 1:\n","                    x[1] = 0\n","            text_a = (line['text'], line['ents'])\n","            label = line['label']\n","            examples.append((guid, text_a, None, label))\n","        return examples\n","\n","    def _read_json(cls, input_file):\n","        with open(input_file, \"r\", encoding='utf-8') as f:\n","            return json.loads(f.read())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EfqOMpJj6Cmf"},"source":["Helper function to convert the loaded training data to useable training instances"]},{"cell_type":"code","metadata":{"id":"uM8GG5IXr_Te"},"source":["def convert_examples_to_features(examples, label_list, max_seq_length, tokenizer):\n","    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n","    \n","    label_list = sorted(label_list)\n","    label_map = {label : i for i, label in enumerate(label_list)}\n","\n","    features = []\n","    for (ex_index, example) in enumerate(examples):\n","        ex_text_a = example[1][0]\n","        h, t = example[1][1]\n","        h_name = ex_text_a[h[1]:h[2]]\n","        t_name = ex_text_a[t[1]:t[2]]\n","        if h[1] < t[1]:\n","            ex_text_a = ex_text_a[:h[1]] + \"[E1] \"+h_name+\" [/E1]\" + ex_text_a[h[2]:t[1]] + \"[E2] \"+t_name+\" [/E2]\" + ex_text_a[t[2]:]\n","            \n","        else:\n","            ex_text_a = ex_text_a[:t[1]] + \"[E2] \"+t_name+\" [/E2]\" + ex_text_a[t[2]:h[1]] + \"[E1] \"+h_name+\" [/E1]\" + ex_text_a[h[2]:]\n","\n","        if h[1] < t[1]:\n","            h[1] += 2\n","            h[2] += 2\n","            t[1] += 6\n","            t[2] += 6\n","        else:\n","            h[1] += 6\n","            h[2] += 6\n","            t[1] += 2\n","            t[2] += 2\n","        \n","        tokens_a = tokenizer.tokenize(ex_text_a)\n","\n","\n","        # Account for [CLS] and [SEP] with \"- 2\"\n","        if len(tokens_a) > max_seq_length - 2:\n","            tokens_a = tokens_a[:(max_seq_length - 2)]\n","\n","        tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"]\n","\n","        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n","        attention_mask = input_mask = [1] * len(input_ids)\n","        token_type_ids = [0] * max_seq_length\n","\n","        # Zero-pad up to the sequence length.\n","        padding = [0] * (max_seq_length - len(input_ids))\n","        input_ids += padding\n","        attention_mask += padding\n","\n","        assert len(input_ids) == max_seq_length\n","\n","        label_id = label_map[example[3]]\n","\n","        features.append((input_ids, label_id, attention_mask, token_type_ids))\n","\n","    return features, label_map"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KdLyhGRI6OpG"},"source":["Helper function for running evaluation during training process"]},{"cell_type":"code","metadata":{"id":"H3IdHsBXWCvf"},"source":["def run_evaluation(dataloader, model):\n","    eval_loss, eval_accuracy, eval_precision, eval_recall, eval_f1 = 0, 0, 0, 0, 0\n","    nb_eval_steps, nb_eval_examples = 0, 0\n","    for dev_input_ids, dev_labels, dev_attention_mask, dev_token_type_ids in dataloader:\n","        dev_input_ids = dev_input_ids.to(device)\n","        dev_labels = dev_labels.to(device)\n","        dev_attention_mask = dev_attention_mask.to(device)\n","        dev_token_type_ids = dev_token_type_ids.to(device)\n","\n","        with torch.no_grad():\n","            tmp_eval_loss, logits = model(dev_input_ids, token_type_ids=dev_token_type_ids, attention_mask=dev_attention_mask, labels=dev_labels)\n","      \n","        logits = logits.detach().cpu().numpy()\n","        dev_labels = dev_labels.to('cpu').numpy()\n","        tmp_eval_accuracy, tmp_eval_precision, tmp_eval_recall, tmp_eval_f1, pred = accuracy_precision_recall_f1(logits, dev_labels)\n","\n","        eval_loss += tmp_eval_loss.mean().item()\n","        eval_accuracy += tmp_eval_accuracy\n","        eval_precision += tmp_eval_precision\n","        eval_recall += tmp_eval_recall\n","        eval_f1 += tmp_eval_f1\n","\n","        nb_eval_examples += dev_input_ids.size(0)\n","        nb_eval_steps += 1\n","\n","    eval_loss = eval_loss / nb_eval_steps\n","    eval_accuracy = eval_accuracy / nb_eval_examples\n","    eval_precision = eval_precision / nb_eval_steps\n","    eval_recall = eval_recall / nb_eval_steps\n","    eval_f1 = eval_f1 / nb_eval_steps\n","\n","    print(\"***** Eval results *****\")\n","    print(\"   Loss: %f\" % eval_loss)\n","    print(\"   Accuracy: %f\" % eval_accuracy)\n","    print(\"   Precision (macro-averaged): %f\" % eval_precision)\n","    print(\"   Recall (macro-averaged): %f\" % eval_recall)\n","    print(\"   F1-Score (macro-averaged): %f\" % eval_f1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SJHWpgPO6UHv"},"source":["Helper function to calculate accuracy during training process"]},{"cell_type":"code","metadata":{"id":"sekatlkxQX1J"},"source":["def accuracy_precision_recall_f1(out, labels):\n","    outputs = np.argmax(out, axis=1)\n","    accuracy = np.sum(outputs == labels)\n","    precision = precision_score(labels, outputs, average='macro', labels=np.unique(labels))\n","    recall = recall_score(labels, outputs, average='macro', labels=np.unique(labels))\n","    f1 = f1_score(labels, outputs, average='macro', labels=np.unique(labels))\n","    return accuracy, precision, recall, f1, outputs"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FGcH1RC2P5w2"},"source":["### Fine-Tuning"]},{"cell_type":"markdown","metadata":{"id":"_uqKVi1e6Zhq"},"source":["Parameter use_pretrained_blanks decides if BERT oder MTB-BERT model is used for fine-tuning"]},{"cell_type":"code","metadata":{"id":"fLWrWxwIpp8B"},"source":["if use_pretrained_blanks == 1:\n","    print(\"Loading model pre-trained on blanks ...\")\n","    checkpoint = torch.load(checkpoint_path, map_location=torch.device(device))\n","    model_dict = model.state_dict()\n","    pretrained_dict = {k: v for k, v in checkpoint['state_dict'].items() if k in model_dict.keys()}\n","    model_dict.update(pretrained_dict)\n","    model.load_state_dict(pretrained_dict, strict=False)\n","    del checkpoint, pretrained_dict, model_dict"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qd8q1YYvoo27"},"source":["criterion = nn.CrossEntropyLoss(ignore_index=-1)\n","optimizer = optim.Adam([{\"params\":model.parameters(), \"lr\": lr}])\n","\n","scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[2,4,6,8,12,15,18,20,22,24,26,30], gamma=0.8)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LH_Mqb096tqw"},"source":["Loading and preparing training data for the fine-tuning process"]},{"cell_type":"code","metadata":{"id":"_gmbBjWnv0Ke","colab":{"base_uri":"https://localhost:8080/","height":68},"executionInfo":{"status":"ok","timestamp":1603355127086,"user_tz":-120,"elapsed":2559,"user":{"displayName":"Christoph Brandl","photoUrl":"","userId":"16180149632937868484"}},"outputId":"c4a310a3-1780-4c03-e364-140959783f37"},"source":["processor = FewrelProcessor()\n","\n","# Prepare training data for fine-tuning\n","train_examples, label_list = processor.get_train_examples(data_dir, train_data_file)\n","train_features, label_map = convert_examples_to_features(train_examples, label_list, max_seq_length, tokenizer)\n","\n","all_input_ids = torch.tensor([f[0] for f in train_features], dtype=torch.long)\n","all_label_ids = torch.tensor([f[1] for f in train_features], dtype=torch.long)\n","all_attention_masks = torch.tensor([f[2] for f in train_features], dtype=torch.long)\n","all_token_type_ids = torch.tensor([f[3] for f in train_features], dtype=torch.long)\n","\n","train_data = TensorDataset(all_input_ids, all_label_ids, all_attention_masks, all_token_type_ids)\n","\n","train_sampler = RandomSampler(train_data)\n","train_loader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","\n","print('Number of train examples: %d' % len(train_examples))\n","\n","\n","# Prepare dev data for evaluation while fine-tuning\n","dev_examples, label_list_dev = processor.get_dev_examples(data_dir, test_data_file)\n","dev_features, label_map_dev = convert_examples_to_features(dev_examples, label_list_dev, max_seq_length, tokenizer)\n","\n","all_input_ids_dev = torch.tensor([f[0] for f in dev_features], dtype=torch.long)\n","all_label_ids_dev = torch.tensor([f[1] for f in dev_features], dtype=torch.long)\n","all_attention_masks_dev = torch.tensor([f[2] for f in dev_features], dtype=torch.long)\n","all_token_type_ids_dev = torch.tensor([f[3] for f in dev_features], dtype=torch.long)\n","\n","dev_data = TensorDataset(all_input_ids_dev, all_label_ids_dev, all_attention_masks_dev, all_token_type_ids_dev)\n","\n","dev_sampler = SequentialSampler(dev_data)\n","dev_dataloader = DataLoader(dev_data, sampler=dev_sampler, batch_size=batch_size)\n","\n","print('Number of evaluation examples: %d' % len(dev_examples))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Number of train examples: 1068\n","Number of evaluation examples: 356\n","time: 2.04 s\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"6ePsPOn460Tg"},"source":["Actual training process of the model"]},{"cell_type":"code","metadata":{"id":"HzdjFTdkpZKI","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1603355226554,"user_tz":-120,"elapsed":97899,"user":{"displayName":"Christoph Brandl","photoUrl":"","userId":"16180149632937868484"}},"outputId":"aed55754-dde4-4503-8fb4-89bd880e0244"},"source":["losses_per_epoch=[]\n","accuracy_per_epoch=[]\n","test_f1_per_epoch=[]\n","save_path = os.path.join(basepath, 'MTB/fine_tuning_checkpoints/')\n","\n","\n","train_len = len(train_data)\n","print(\"Starting training process...\")\n","\n","label_map_file = os.path.join(save_path, 'label_map_%s_%d_classes_examples.json' %(data_type, num_classes))\n","with open(label_map_file, 'w') as f:\n","    json.dump(label_map, f)\n","\n","update_size = len(train_loader)//10\n","\n","for epoch in range(0, num_epochs):\n","    print(\"------------- Epoch %d -------------\" % (epoch+1))\n","    start_time = time.time()\n","    model.train(); total_loss = 0.0; losses_per_batch = []; total_acc = 0.0; accuracy_per_batch = []\n","    for i, data in enumerate(train_loader, 0):\n","\n","        x, labels, attention_mask, token_type_ids = data\n","\n","        if torch.cuda.is_available():\n","            x = x.cuda()\n","            labels = labels.cuda()\n","            attention_mask = attention_mask.cuda()\n","            token_type_ids = token_type_ids.cuda()\n","            \n","        loss, classification_logits = model(x, token_type_ids=token_type_ids, attention_mask=attention_mask, labels=labels)\n","        loss = loss/gradient_acc_steps\n","\n","        loss.backward()\n","        \n","        if (i % gradient_acc_steps) == 0:\n","            optimizer.step()\n","            optimizer.zero_grad()\n","        \n","        total_loss += loss.item()\n","        total_acc += evaluate_(classification_logits, labels, \\\n","                                ignore_idx=-1)[0]\n","        \n","        if (i % update_size) == (update_size - 1):\n","            losses_per_batch.append(gradient_acc_steps*total_loss/update_size)\n","            accuracy_per_batch.append(total_acc/update_size)\n","\n","            print('[Epoch: %d, %5d/ %d points] total loss, accuracy per batch: %.3f, %.3f' %\n","                  (epoch + 1, (i + 1)*batch_size, train_len, losses_per_batch[-1], accuracy_per_batch[-1]))\n","            \n","            total_loss = 0.0; total_acc = 0.0\n","\n","    scheduler.step()\n","\n","    losses_per_epoch.append(sum(losses_per_batch)/len(losses_per_batch))\n","    accuracy_per_epoch.append(sum(accuracy_per_batch)/len(accuracy_per_batch))\n","\n","    print(\"Epoch finished, took %.2f seconds.\" % (time.time() - start_time))\n","    print(\"Losses at Epoch %d: %.7f\" % (epoch + 1, losses_per_epoch[-1]))\n","    print(\"Train accuracy at Epoch %d: %.7f\" % (epoch + 1, accuracy_per_epoch[-1]))\n","\n","    print(\"***** Running evaluation on Dev data *****\")\n","    print(\"   Num examples = %d\" % len(dev_examples))\n","    run_evaluation(dev_dataloader, model)\n","\n","\n","mtb = 'no_MTB'\n","if (use_pretrained_blanks==1):\n","    mtb = 'MTB'\n","\n","# Save a trained model\n","model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n","output_model_file = os.path.join(save_path, \"fine_tuning_checkpoint_BERT_%s_%s_%d_classes.pth.tar\" % (mtb, data_type, num_classes))\n","torch.save(model_to_save.state_dict(), output_model_file)\n","    \n","print(\"Finished Training!\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Starting training process...\n","------------- Epoch 1 -------------\n","[Epoch: 1,    96/ 1068 points] total loss, accuracy per batch: 1.923, 0.146\n","[Epoch: 1,   192/ 1068 points] total loss, accuracy per batch: 1.962, 0.177\n","[Epoch: 1,   288/ 1068 points] total loss, accuracy per batch: 1.891, 0.156\n","[Epoch: 1,   384/ 1068 points] total loss, accuracy per batch: 1.866, 0.292\n","[Epoch: 1,   480/ 1068 points] total loss, accuracy per batch: 1.907, 0.198\n","[Epoch: 1,   576/ 1068 points] total loss, accuracy per batch: 1.887, 0.198\n","[Epoch: 1,   672/ 1068 points] total loss, accuracy per batch: 1.836, 0.188\n","[Epoch: 1,   768/ 1068 points] total loss, accuracy per batch: 1.759, 0.323\n","[Epoch: 1,   864/ 1068 points] total loss, accuracy per batch: 1.674, 0.333\n","[Epoch: 1,   960/ 1068 points] total loss, accuracy per batch: 1.756, 0.385\n","[Epoch: 1,  1056/ 1068 points] total loss, accuracy per batch: 1.668, 0.344\n","Epoch finished, took 10.86 seconds.\n","Losses at Epoch 1: 1.8300044\n","Train accuracy at Epoch 1: 0.2490530\n","***** Running evaluation on Dev data *****\n","   Num examples = 356\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"],"name":"stderr"},{"output_type":"stream","text":["***** Eval results *****\n","   Loss: 1.734531\n","   Accuracy: 0.292135\n","   Precision (macro-averaged): 0.211494\n","   Recall (macro-averaged): 0.219161\n","   F1-Score (macro-averaged): 0.167689\n","------------- Epoch 2 -------------\n","[Epoch: 2,    96/ 1068 points] total loss, accuracy per batch: 1.629, 0.354\n","[Epoch: 2,   192/ 1068 points] total loss, accuracy per batch: 1.414, 0.583\n","[Epoch: 2,   288/ 1068 points] total loss, accuracy per batch: 1.396, 0.500\n","[Epoch: 2,   384/ 1068 points] total loss, accuracy per batch: 1.396, 0.490\n","[Epoch: 2,   480/ 1068 points] total loss, accuracy per batch: 1.298, 0.500\n","[Epoch: 2,   576/ 1068 points] total loss, accuracy per batch: 1.304, 0.490\n","[Epoch: 2,   672/ 1068 points] total loss, accuracy per batch: 1.329, 0.510\n","[Epoch: 2,   768/ 1068 points] total loss, accuracy per batch: 1.257, 0.490\n","[Epoch: 2,   864/ 1068 points] total loss, accuracy per batch: 1.198, 0.458\n","[Epoch: 2,   960/ 1068 points] total loss, accuracy per batch: 1.172, 0.448\n","[Epoch: 2,  1056/ 1068 points] total loss, accuracy per batch: 1.136, 0.531\n","Epoch finished, took 10.64 seconds.\n","Losses at Epoch 2: 1.3206610\n","Train accuracy at Epoch 2: 0.4867424\n","***** Running evaluation on Dev data *****\n","   Num examples = 356\n","***** Eval results *****\n","   Loss: 1.152292\n","   Accuracy: 0.547753\n","   Precision (macro-averaged): 0.543172\n","   Recall (macro-averaged): 0.578017\n","   F1-Score (macro-averaged): 0.504753\n","------------- Epoch 3 -------------\n","[Epoch: 3,    96/ 1068 points] total loss, accuracy per batch: 1.069, 0.573\n","[Epoch: 3,   192/ 1068 points] total loss, accuracy per batch: 1.024, 0.562\n","[Epoch: 3,   288/ 1068 points] total loss, accuracy per batch: 0.838, 0.729\n","[Epoch: 3,   384/ 1068 points] total loss, accuracy per batch: 0.913, 0.688\n","[Epoch: 3,   480/ 1068 points] total loss, accuracy per batch: 1.069, 0.562\n","[Epoch: 3,   576/ 1068 points] total loss, accuracy per batch: 0.842, 0.656\n","[Epoch: 3,   672/ 1068 points] total loss, accuracy per batch: 0.963, 0.646\n","[Epoch: 3,   768/ 1068 points] total loss, accuracy per batch: 0.910, 0.625\n","[Epoch: 3,   864/ 1068 points] total loss, accuracy per batch: 0.816, 0.688\n","[Epoch: 3,   960/ 1068 points] total loss, accuracy per batch: 0.871, 0.677\n","[Epoch: 3,  1056/ 1068 points] total loss, accuracy per batch: 0.930, 0.698\n","Epoch finished, took 10.64 seconds.\n","Losses at Epoch 3: 0.9312585\n","Train accuracy at Epoch 3: 0.6458333\n","***** Running evaluation on Dev data *****\n","   Num examples = 356\n","***** Eval results *****\n","   Loss: 1.084118\n","   Accuracy: 0.550562\n","   Precision (macro-averaged): 0.522554\n","   Recall (macro-averaged): 0.521495\n","   F1-Score (macro-averaged): 0.475794\n","------------- Epoch 4 -------------\n","[Epoch: 4,    96/ 1068 points] total loss, accuracy per batch: 0.742, 0.688\n","[Epoch: 4,   192/ 1068 points] total loss, accuracy per batch: 0.695, 0.760\n","[Epoch: 4,   288/ 1068 points] total loss, accuracy per batch: 0.632, 0.750\n","[Epoch: 4,   384/ 1068 points] total loss, accuracy per batch: 0.683, 0.781\n","[Epoch: 4,   480/ 1068 points] total loss, accuracy per batch: 0.615, 0.833\n","[Epoch: 4,   576/ 1068 points] total loss, accuracy per batch: 0.633, 0.802\n","[Epoch: 4,   672/ 1068 points] total loss, accuracy per batch: 0.714, 0.750\n","[Epoch: 4,   768/ 1068 points] total loss, accuracy per batch: 0.616, 0.771\n","[Epoch: 4,   864/ 1068 points] total loss, accuracy per batch: 0.543, 0.823\n","[Epoch: 4,   960/ 1068 points] total loss, accuracy per batch: 0.468, 0.885\n","[Epoch: 4,  1056/ 1068 points] total loss, accuracy per batch: 0.536, 0.854\n","Epoch finished, took 10.63 seconds.\n","Losses at Epoch 4: 0.6253689\n","Train accuracy at Epoch 4: 0.7907197\n","***** Running evaluation on Dev data *****\n","   Num examples = 356\n","***** Eval results *****\n","   Loss: 0.940918\n","   Accuracy: 0.654494\n","   Precision (macro-averaged): 0.627639\n","   Recall (macro-averaged): 0.647793\n","   F1-Score (macro-averaged): 0.604233\n","------------- Epoch 5 -------------\n","[Epoch: 5,    96/ 1068 points] total loss, accuracy per batch: 0.510, 0.844\n","[Epoch: 5,   192/ 1068 points] total loss, accuracy per batch: 0.393, 0.896\n","[Epoch: 5,   288/ 1068 points] total loss, accuracy per batch: 0.351, 0.896\n","[Epoch: 5,   384/ 1068 points] total loss, accuracy per batch: 0.425, 0.875\n","[Epoch: 5,   480/ 1068 points] total loss, accuracy per batch: 0.441, 0.875\n","[Epoch: 5,   576/ 1068 points] total loss, accuracy per batch: 0.361, 0.875\n","[Epoch: 5,   672/ 1068 points] total loss, accuracy per batch: 0.442, 0.885\n","[Epoch: 5,   768/ 1068 points] total loss, accuracy per batch: 0.393, 0.875\n","[Epoch: 5,   864/ 1068 points] total loss, accuracy per batch: 0.342, 0.885\n","[Epoch: 5,   960/ 1068 points] total loss, accuracy per batch: 0.422, 0.854\n","[Epoch: 5,  1056/ 1068 points] total loss, accuracy per batch: 0.366, 0.906\n","Epoch finished, took 10.64 seconds.\n","Losses at Epoch 5: 0.4041671\n","Train accuracy at Epoch 5: 0.8787879\n","***** Running evaluation on Dev data *****\n","   Num examples = 356\n","***** Eval results *****\n","   Loss: 0.880498\n","   Accuracy: 0.691011\n","   Precision (macro-averaged): 0.702386\n","   Recall (macro-averaged): 0.726827\n","   F1-Score (macro-averaged): 0.682311\n","------------- Epoch 6 -------------\n","[Epoch: 6,    96/ 1068 points] total loss, accuracy per batch: 0.295, 0.906\n","[Epoch: 6,   192/ 1068 points] total loss, accuracy per batch: 0.331, 0.906\n","[Epoch: 6,   288/ 1068 points] total loss, accuracy per batch: 0.257, 0.917\n","[Epoch: 6,   384/ 1068 points] total loss, accuracy per batch: 0.221, 0.938\n","[Epoch: 6,   480/ 1068 points] total loss, accuracy per batch: 0.335, 0.906\n","[Epoch: 6,   576/ 1068 points] total loss, accuracy per batch: 0.268, 0.927\n","[Epoch: 6,   672/ 1068 points] total loss, accuracy per batch: 0.132, 0.979\n","[Epoch: 6,   768/ 1068 points] total loss, accuracy per batch: 0.224, 0.938\n","[Epoch: 6,   864/ 1068 points] total loss, accuracy per batch: 0.245, 0.927\n","[Epoch: 6,   960/ 1068 points] total loss, accuracy per batch: 0.284, 0.927\n","[Epoch: 6,  1056/ 1068 points] total loss, accuracy per batch: 0.192, 0.958\n","Epoch finished, took 10.65 seconds.\n","Losses at Epoch 6: 0.2532712\n","Train accuracy at Epoch 6: 0.9299242\n","***** Running evaluation on Dev data *****\n","   Num examples = 356\n","***** Eval results *****\n","   Loss: 0.933386\n","   Accuracy: 0.688202\n","   Precision (macro-averaged): 0.668353\n","   Recall (macro-averaged): 0.691459\n","   F1-Score (macro-averaged): 0.645249\n","------------- Epoch 7 -------------\n","[Epoch: 7,    96/ 1068 points] total loss, accuracy per batch: 0.242, 0.948\n","[Epoch: 7,   192/ 1068 points] total loss, accuracy per batch: 0.140, 0.969\n","[Epoch: 7,   288/ 1068 points] total loss, accuracy per batch: 0.242, 0.917\n","[Epoch: 7,   384/ 1068 points] total loss, accuracy per batch: 0.150, 0.958\n","[Epoch: 7,   480/ 1068 points] total loss, accuracy per batch: 0.126, 0.969\n","[Epoch: 7,   576/ 1068 points] total loss, accuracy per batch: 0.123, 0.969\n","[Epoch: 7,   672/ 1068 points] total loss, accuracy per batch: 0.113, 0.979\n","[Epoch: 7,   768/ 1068 points] total loss, accuracy per batch: 0.112, 0.990\n","[Epoch: 7,   864/ 1068 points] total loss, accuracy per batch: 0.149, 0.948\n","[Epoch: 7,   960/ 1068 points] total loss, accuracy per batch: 0.184, 0.927\n","[Epoch: 7,  1056/ 1068 points] total loss, accuracy per batch: 0.201, 0.938\n","Epoch finished, took 10.64 seconds.\n","Losses at Epoch 7: 0.1620431\n","Train accuracy at Epoch 7: 0.9554924\n","***** Running evaluation on Dev data *****\n","   Num examples = 356\n","***** Eval results *****\n","   Loss: 0.865346\n","   Accuracy: 0.730337\n","   Precision (macro-averaged): 0.751679\n","   Recall (macro-averaged): 0.760025\n","   F1-Score (macro-averaged): 0.726139\n","Finished Training!\n","time: 1min 37s\n"],"name":"stdout"}]}]}