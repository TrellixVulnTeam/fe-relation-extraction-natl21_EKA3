{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ERNIE Relation Extraction.ipynb","provenance":[{"file_id":"1_i6d5uaAiqpuUwvf5NiHxcXHf58kAqt1","timestamp":1591254410656}],"collapsed_sections":[],"machine_shape":"hm","mount_file_id":"17yGp5vjcTs0lEV6ecS7bqXXSa2AjcEUq","authorship_tag":"ABX9TyNBLbiGxSJgmUIVZIssYkB5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"GWRgjzuAWNU0"},"source":["# **ERNIE Relation Extraction Notebook**"]},{"cell_type":"markdown","metadata":{"id":"7Ho6EN-cnTat"},"source":["## Imports and environment configuration"]},{"cell_type":"code","metadata":{"id":"35wKKgtuWikV"},"source":["!pip install ipython-autotime\n","!pip install boto3\n","!pip install simplejson\n","\n","%load_ext autotime"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nPZw9mAzPX7d"},"source":["Imports"]},{"cell_type":"code","metadata":{"id":"WNRRDsDGWjZ_"},"source":["import sys\n","import os\n","import re\n","import random\n","import time\n","from pathlib import Path\n","import numpy as np\n","import json\n","import torch\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","from torch.utils.data.distributed import DistributedSampler\n","from sklearn.metrics import f1_score, precision_score, recall_score\n","\n","if 'google.colab' in str(get_ipython()):\n","  print('Running on Google Colab')\n","  root = '/content/drive/My Drive/Colab Notebooks/'\n","else:\n","  print('Running locally')\n","  root = Path(os.getcwd()).parent\n","\n","basepath = os.path.join(root, 'relation-extraction/')\n","sys.path.append(os.path.join(basepath, 'ERNIE/code'))\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","n_gpu = torch.cuda.device_count()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s_0u_02UWlyl"},"source":["from knowledge_bert.tokenization import BertTokenizer\n","from knowledge_bert.modeling import BertForSequenceClassification\n","from knowledge_bert.optimization import BertAdam\n","from knowledge_bert.file_utils import PYTORCH_PRETRAINED_BERT_CACHE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RLpt4eIeEb4C"},"source":["Switch for data usage: If True FewRel data will be used, if False Future Engineering data is used"]},{"cell_type":"code","metadata":{"id":"Uny2n32SEYDv"},"source":["use_fewrel_data=False"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cgeYNGT4PbUR"},"source":["Initialization of parameters and data paths"]},{"cell_type":"code","metadata":{"id":"AXNoquUUWr72"},"source":["ernie_model = os.path.join(basepath, 'ERNIE/ernie_base')\n","\n","max_seq_length = 256\n","do_train = True\n","do_eval = False\n","do_lower_case = True\n","train_batch_size = 32\n","eval_batch_size = 8\n","learning_rate = 2e-5\n","num_train_epochs = 4\n","warmup_proportion = 0.1\n","local_rank = -1\n","seed = 42\n","gradient_accumulation_steps = 1\n","fp16 = False\n","loss_scale = 128\n","threshold = 0.3\n","\n","num_labels_task = 7\n","\n","if (use_fewrel_data):\n","    output_dir = os.path.join(basepath, 'ERNIE/output_fewrel_test')\n","\n","    data_dir = os.path.join(root, 'fewrel-training-data/fewrel/')\n","    test_data_file = \"test_%d_classes_disjoint.json\" % num_labels_task\n","    train_data_file = \"dev_%d_classes_disjoint.json\" % num_labels_task\n","\n","    data_type = 'fewrel'\n","else:\n","    output_dir = os.path.join(basepath, 'ERNIE/output_fe')\n","    \n","    data_dir = os.path.join(root, 'fe-training-data')\n","    test_data_file = 'test_examples_nota_manufact_operate_operatesth_order_uses_ordersth.json'\n","    train_data_file = 'train_examples_nota_manufact_operate_operatesth_order_uses_ordersth.json'\n","\n","    data_type = 'fe'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1hKvbHpEPhTx"},"source":["Loading of pre-trained entity embeddings which are provided by the authors of ERNIE approach."]},{"cell_type":"code","metadata":{"id":"vbmYksHASKwZ","colab":{"base_uri":"https://localhost:8080/","height":153},"executionInfo":{"status":"ok","timestamp":1603872173703,"user_tz":-60,"elapsed":405159,"user":{"displayName":"Christoph Brandl","photoUrl":"","userId":"16180149632937868484"}},"outputId":"e9b2d4af-ef14-4988-982a-91b5a485b727"},"source":["vecs = []\n","vecs.append([0]*100)\n","with open(os.path.join(basepath, \"ERNIE/kg_embed/entity2vec.vec\"), 'r') as fin:\n","    for i, line in enumerate(fin):\n","        vec = line.strip().split('\\t')\n","        vec = [float(x) for x in vec]\n","        vecs.append(vec)\n","        if (i % 1000000==0):\n","            print('Processed %d lines' % i)\n","\n","embed = torch.FloatTensor(vecs)\n","embed = torch.nn.Embedding.from_pretrained(embed)\n","\n","print(\"Shape of entity embedding: \" + str(embed.weight.size()))\n","del vecs"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Processed 0 lines\n","Processed 1000000 lines\n","Processed 2000000 lines\n","Processed 3000000 lines\n","Processed 4000000 lines\n","Processed 5000000 lines\n","Shape of entity embedding: torch.Size([5040987, 100])\n","time: 6min 44s\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"JNZU5iANc7Rc"},"source":["## Helper functions and commonly needed elements"]},{"cell_type":"markdown","metadata":{"id":"_NEvvruOPrXo"},"source":["Helper function for running evaluation while fine-tuning"]},{"cell_type":"code","metadata":{"id":"fn0veXZxB18W"},"source":["def run_evaluation(dataloader, model):\n","    eval_loss, eval_accuracy, eval_precision, eval_recall, eval_f1 = 0, 0, 0, 0, 0\n","    nb_eval_steps, nb_eval_examples = 0, 0\n","    for input_ids, input_mask, segment_ids, input_ent, ent_mask, label_ids in dataloader:\n","        input_ent = embed(input_ent+1) # -1 -> 0\n","        input_ids = input_ids.to(device)\n","        input_mask = input_mask.to(device)\n","        segment_ids = segment_ids.to(device)\n","        input_ent = input_ent.to(device)\n","        ent_mask = ent_mask.to(device)\n","        label_ids = label_ids.to(device)\n","\n","        with torch.no_grad():\n","            tmp_eval_loss = model(input_ids, segment_ids, input_mask, input_ent, ent_mask, label_ids)\n","            logits = model(input_ids, segment_ids, input_mask, input_ent, ent_mask)\n","\n","        logits = logits.detach().cpu().numpy()\n","        label_ids = label_ids.to('cpu').numpy()\n","        tmp_eval_accuracy, tmp_eval_precision, tmp_eval_recall, tmp_eval_f1, pred = accuracy_precision_recall_f1(logits, label_ids)\n","\n","        eval_loss += tmp_eval_loss.mean().item()\n","        eval_accuracy += tmp_eval_accuracy\n","        eval_precision += tmp_eval_precision\n","        eval_recall += tmp_eval_recall\n","        eval_f1 += tmp_eval_f1\n","\n","        nb_eval_examples += input_ids.size(0)\n","        nb_eval_steps += 1\n","\n","    eval_loss = eval_loss / nb_eval_steps\n","    eval_accuracy = eval_accuracy / nb_eval_examples\n","    eval_precision = eval_precision / nb_eval_steps\n","    eval_recall = eval_recall / nb_eval_steps\n","    eval_f1 = eval_f1 / nb_eval_steps\n","\n","    print(\"***** Eval results *****\")\n","    print(\"   Loss: %f\" % eval_loss)\n","    print(\"   Accuracy: %f\" % eval_accuracy)\n","    print(\"   Precision (macro-averaged): %f\" % eval_precision)\n","    print(\"   Recall (macro-averaged): %f\" % eval_recall)\n","    print(\"   F1-Score (macro-averaged): %f\" % eval_f1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NslxSNkBPvjF"},"source":["Helper function to calculate accuracy while fine-tuning"]},{"cell_type":"code","metadata":{"id":"9GsDPmSfCWQU"},"source":["def accuracy_precision_recall_f1(out, labels):\n","    outputs = np.argmax(out, axis=1)\n","    accuracy = np.sum(outputs == labels)\n","    precision = precision_score(labels, outputs, average='macro', labels=np.unique(labels))\n","    recall = recall_score(labels, outputs, average='macro', labels=np.unique(labels))\n","    f1 = f1_score(labels, outputs, average='macro', labels=np.unique(labels))\n","    return accuracy, precision, recall, f1, outputs"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ff2DAz1anfVL"},"source":["## Fine-Tuning"]},{"cell_type":"code","metadata":{"id":"bdmpeFJhWq36"},"source":["from run_fewrel import InputExample, InputFeatures, DataProcessor, FewrelProcessor\n","from run_fewrel import convert_examples_to_features, accuracy, warmup_linear"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ytWhRK1bP0JI"},"source":["Preparing needed information for fine-tuning and setting random seeds"]},{"cell_type":"code","metadata":{"id":"1JJAu5zdWuXA"},"source":["train_batch_size = int(train_batch_size / gradient_accumulation_steps)\n","\n","random.seed(seed)\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n","\n","if n_gpu > 0:\n","    torch.cuda.manual_seed_all(seed)\n","\n","os.makedirs(output_dir, exist_ok=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p-Oj_cDfQIhA"},"source":["Preparing tokenizer and model for fine-tuning"]},{"cell_type":"code","metadata":{"id":"QJBa3ZW7uvrG"},"source":["tokenizer = BertTokenizer.from_pretrained(ernie_model, do_lower_case=do_lower_case)\n","\n","# Prepare model\n","model, _ = BertForSequenceClassification.from_pretrained(ernie_model,\n","          cache_dir=PYTORCH_PRETRAINED_BERT_CACHE / 'distributed_{}'.format(local_rank),\n","          num_labels = num_labels_task)\n","\n","model.to(device)\n","\n","if n_gpu > 1:\n","    model = torch.nn.DataParallel(model)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e4q-an2LQfgF"},"source":["Loading training and evaluation data"]},{"cell_type":"code","metadata":{"id":"dbbS8i6hs0Wl","colab":{"base_uri":"https://localhost:8080/","height":68},"executionInfo":{"status":"ok","timestamp":1603874988328,"user_tz":-60,"elapsed":10096,"user":{"displayName":"Christoph Brandl","photoUrl":"","userId":"16180149632937868484"}},"outputId":"7a1dc0aa-0d62-4ff4-ae51-af3259ddc27d"},"source":["processor = FewrelProcessor()\n","\n","# Prepare train data for fine-tuning\n","train_examples, label_list = processor.get_train_examples(data_dir, train_data_file)\n","\n","train_features, label_map = convert_examples_to_features(train_examples, label_list, max_seq_length, tokenizer, threshold, os.path.join(basepath, \"ERNIE/kg_embed/entity2id.txt\"))\n","\n","all_input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long)\n","all_input_mask = torch.tensor([f.input_mask for f in train_features], dtype=torch.long)\n","all_segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype=torch.long)\n","all_label_ids = torch.tensor([f.label_id for f in train_features], dtype=torch.long)\n","all_ent = torch.tensor([f.input_ent for f in train_features], dtype=torch.long)\n","all_ent_masks = torch.tensor([f.ent_mask for f in train_features], dtype=torch.long)\n","\n","train_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_ent, all_ent_masks, all_label_ids)\n","\n","if local_rank == -1:\n","    train_sampler = RandomSampler(train_data)\n","else:\n","    train_sampler = DistributedSampler(train_data)\n","\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=train_batch_size)\n","\n","print('Number of training examples: %d' % len(train_examples))\n","\n","#Prepare evaluation data for fine-tuning\n","dev_examples = processor.get_dev_examples(data_dir, test_data_file)\n","dev_features,_ = convert_examples_to_features(dev_examples, label_list, max_seq_length, tokenizer, threshold,os.path.join(basepath, \"ERNIE/kg_embed/entity2id.txt\"))\n","\n","all_input_ids_dev = torch.tensor([f.input_ids for f in dev_features], dtype=torch.long)\n","all_input_mask_dev = torch.tensor([f.input_mask for f in dev_features], dtype=torch.long)\n","all_segment_ids_dev = torch.tensor([f.segment_ids for f in dev_features], dtype=torch.long)\n","all_label_ids_dev = torch.tensor([f.label_id for f in dev_features], dtype=torch.long)\n","all_ent_dev = torch.tensor([f.input_ent for f in dev_features], dtype=torch.long)\n","all_ent_masks_dev = torch.tensor([f.ent_mask for f in dev_features], dtype=torch.long)\n","\n","dev_data = TensorDataset(all_input_ids_dev, all_input_mask_dev, all_segment_ids_dev, all_ent_dev, all_ent_masks_dev, all_label_ids_dev)\n","\n","# Run prediction for full data\n","dev_sampler = SequentialSampler(dev_data)\n","dev_dataloader = DataLoader(dev_data, sampler=dev_sampler, batch_size=train_batch_size)\n","\n","print('Number of evaluation examples: %d' % len(dev_examples))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Number of training examples: 1068\n","Number of evaluation examples: 356\n","time: 9.34 s\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"8auR7QxgRWSm"},"source":["Preparing optimizer"]},{"cell_type":"code","metadata":{"id":"wGjD_fZAWw0S"},"source":["num_train_steps = int(\n","    len(train_examples) / train_batch_size / gradient_accumulation_steps * num_train_epochs)\n","\n","# Prepare optimizer\n","param_optimizer = list(model.named_parameters())\n","no_grad = ['bert.encoder.layer.11.output.dense_ent', 'bert.encoder.layer.11.output.LayerNorm_ent']\n","param_optimizer = [(n, p) for n, p in param_optimizer if not any(nd in n for nd in no_grad)]\n","no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n","optimizer_grouped_parameters = [\n","    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n","    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n","    ]\n","t_total = num_train_steps\n","\n","optimizer = BertAdam(optimizer_grouped_parameters,\n","                      lr=learning_rate,\n","                      warmup=warmup_proportion,\n","                      t_total=t_total)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HEkDfjz7RdsN"},"source":["Actual training process for fine-tuning"]},{"cell_type":"code","metadata":{"id":"cHdW7dnLW4lR","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1603875215401,"user_tz":-60,"elapsed":176738,"user":{"displayName":"Christoph Brandl","photoUrl":"","userId":"16180149632937868484"}},"outputId":"5048a1d4-873b-4c56-b5e0-4612f548f7f5"},"source":["global_step = 0\n","\n","with open(os.path.join(output_dir, 'label_map_%s_%d_classes.json' %(data_type, num_labels_task)), 'w') as f:\n","    json.dump(label_map, f)\n","\n","print(\"***** Running training *****\")\n","print(\"  Num examples = %d\" % len(train_examples))\n","print(\"  Batch size = %d\" % train_batch_size)\n","print(\"  Num steps = %d\" % num_train_steps)\n","\n","all_input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long)\n","all_input_mask = torch.tensor([f.input_mask for f in train_features], dtype=torch.long)\n","all_segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype=torch.long)\n","all_label_ids = torch.tensor([f.label_id for f in train_features], dtype=torch.long)\n","all_ent = torch.tensor([f.input_ent for f in train_features], dtype=torch.long)\n","all_ent_masks = torch.tensor([f.ent_mask for f in train_features], dtype=torch.long)\n","\n","train_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_ent, all_ent_masks, all_label_ids)\n","\n","if local_rank == -1:\n","    train_sampler = RandomSampler(train_data)\n","else:\n","    train_sampler = DistributedSampler(train_data)\n","\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=train_batch_size)\n","\n","train_len = len(train_data)\n","update_size = len(train_dataloader)//10\n","\n","model.train()\n","\n","for epoch in range(num_train_epochs):\n","    print(\"------------- Epoch %d -------------\" % (epoch+1))\n","    start_time = time.time()\n","    \n","    tr_loss = 0\n","    tr_steps = 0\n","    for step, batch in enumerate(train_dataloader):\n","        batch = tuple(t.to(device) if i != 3 else t for i, t in enumerate(batch))\n","        input_ids, input_mask, segment_ids, input_ent, ent_mask, label_ids = batch\n","        input_ent = embed(input_ent+1).to(device) # -1 -> 0\n","\n","        loss = model(input_ids, segment_ids, input_mask, input_ent.half(), ent_mask, label_ids)\n","        \n","        if n_gpu > 1:\n","            loss = loss.mean() # mean() to average on multi-gpu.\n","        if gradient_accumulation_steps > 1:\n","            loss = loss / gradient_accumulation_steps\n","\n","        loss.backward()\n","\n","        tr_loss += loss.item()\n","        tr_steps += 1\n","\n","        if (step + 1) % gradient_accumulation_steps == 0:\n","            # modify learning rate with special warm up BERT uses\n","            lr_this_step = learning_rate * warmup_linear(global_step/t_total, warmup_proportion)\n","            for param_group in optimizer.param_groups:\n","                param_group['lr'] = lr_this_step\n","            optimizer.step()\n","            optimizer.zero_grad()\n","            global_step += 1\n","\n","        if (step % update_size) == (update_size - 1):\n","            batch_loss = tr_loss/(update_size * step)\n","            print('[Epoch: %d, %5d/ %d points] loss for batch: %.3f' % (epoch + 1, (step + 1)*train_batch_size, train_len, batch_loss))\n","            \n","    print(\"Epoch finished, took %.2f seconds.\" % (time.time() - start_time))\n","    print(\"Cumulated loss for epoch: %f\" % (tr_loss/tr_steps))\n","\n","    print(\"***** Running evaluation *****\")\n","    print(\"  Num examples = %d\", len(dev_examples))\n","    print(\"  Batch size = %d\", train_batch_size)\n","    run_evaluation(dev_dataloader, model)\n","\n","# Save a trained model\n","model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n","output_model_file = os.path.join(output_dir, \"pytorch_model_%s_%d_classes.bin\" % (data_type, num_labels_task))\n","torch.save(model_to_save.state_dict(), output_model_file)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["***** Running training *****\n","  Num examples = 1068\n","  Batch size = 32\n","  Num steps = 133\n","------------- Epoch 1 -------------\n"],"name":"stdout"},{"output_type":"stream","text":["/content/drive/My Drive/Colab Notebooks/relation-extraction/ERNIE/code/knowledge_bert/optimization.py:132: UserWarning: This overload of add_ is deprecated:\n","\tadd_(Number alpha, Tensor other)\n","Consider using one of the following signatures instead:\n","\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n","  next_m.mul_(beta1).add_(1 - beta1, grad)\n"],"name":"stderr"},{"output_type":"stream","text":["[Epoch: 1,    96/ 1068 points] loss for batch: 1.017\n","[Epoch: 1,   192/ 1068 points] loss for batch: 0.802\n","[Epoch: 1,   288/ 1068 points] loss for batch: 0.740\n","[Epoch: 1,   384/ 1068 points] loss for batch: 0.709\n","[Epoch: 1,   480/ 1068 points] loss for batch: 0.689\n","[Epoch: 1,   576/ 1068 points] loss for batch: 0.681\n","[Epoch: 1,   672/ 1068 points] loss for batch: 0.677\n","[Epoch: 1,   768/ 1068 points] loss for batch: 0.672\n","[Epoch: 1,   864/ 1068 points] loss for batch: 0.667\n","[Epoch: 1,   960/ 1068 points] loss for batch: 0.662\n","[Epoch: 1,  1056/ 1068 points] loss for batch: 0.659\n","Epoch finished, took 35.92 seconds.\n","Cumulated loss for epoch: 1.914726\n","***** Running evaluation *****\n","  Num examples = %d 356\n","  Batch size = %d 32\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"],"name":"stderr"},{"output_type":"stream","text":["***** Eval results *****\n","   Loss: 1.929131\n","   Accuracy: 0.230337\n","   Precision (macro-averaged): 0.062378\n","   Recall (macro-averaged): 0.141251\n","   F1-Score (macro-averaged): 0.064004\n","------------- Epoch 2 -------------\n","[Epoch: 2,    96/ 1068 points] loss for batch: 0.985\n","[Epoch: 2,   192/ 1068 points] loss for batch: 0.780\n","[Epoch: 2,   288/ 1068 points] loss for batch: 0.722\n","[Epoch: 2,   384/ 1068 points] loss for batch: 0.694\n","[Epoch: 2,   480/ 1068 points] loss for batch: 0.681\n","[Epoch: 2,   576/ 1068 points] loss for batch: 0.677\n","[Epoch: 2,   672/ 1068 points] loss for batch: 0.670\n","[Epoch: 2,   768/ 1068 points] loss for batch: 0.663\n","[Epoch: 2,   864/ 1068 points] loss for batch: 0.660\n","[Epoch: 2,   960/ 1068 points] loss for batch: 0.656\n","[Epoch: 2,  1056/ 1068 points] loss for batch: 0.654\n","Epoch finished, took 35.67 seconds.\n","Cumulated loss for epoch: 1.900312\n","***** Running evaluation *****\n","  Num examples = %d 356\n","  Batch size = %d 32\n","***** Eval results *****\n","   Loss: 1.890708\n","   Accuracy: 0.244382\n","   Precision (macro-averaged): 0.074155\n","   Recall (macro-averaged): 0.162600\n","   F1-Score (macro-averaged): 0.094174\n","------------- Epoch 3 -------------\n","[Epoch: 3,    96/ 1068 points] loss for batch: 0.952\n","[Epoch: 3,   192/ 1068 points] loss for batch: 0.756\n","[Epoch: 3,   288/ 1068 points] loss for batch: 0.707\n","[Epoch: 3,   384/ 1068 points] loss for batch: 0.681\n","[Epoch: 3,   480/ 1068 points] loss for batch: 0.670\n","[Epoch: 3,   576/ 1068 points] loss for batch: 0.662\n","[Epoch: 3,   672/ 1068 points] loss for batch: 0.654\n","[Epoch: 3,   768/ 1068 points] loss for batch: 0.646\n","[Epoch: 3,   864/ 1068 points] loss for batch: 0.641\n","[Epoch: 3,   960/ 1068 points] loss for batch: 0.636\n","[Epoch: 3,  1056/ 1068 points] loss for batch: 0.632\n","Epoch finished, took 35.63 seconds.\n","Cumulated loss for epoch: 1.840059\n","***** Running evaluation *****\n","  Num examples = %d 356\n","  Batch size = %d 32\n","***** Eval results *****\n","   Loss: 1.727067\n","   Accuracy: 0.384831\n","   Precision (macro-averaged): 0.292786\n","   Recall (macro-averaged): 0.311109\n","   F1-Score (macro-averaged): 0.262567\n","------------- Epoch 4 -------------\n","[Epoch: 4,    96/ 1068 points] loss for batch: 0.828\n","[Epoch: 4,   192/ 1068 points] loss for batch: 0.674\n","[Epoch: 4,   288/ 1068 points] loss for batch: 0.624\n","[Epoch: 4,   384/ 1068 points] loss for batch: 0.594\n","[Epoch: 4,   480/ 1068 points] loss for batch: 0.577\n","[Epoch: 4,   576/ 1068 points] loss for batch: 0.576\n","[Epoch: 4,   672/ 1068 points] loss for batch: 0.568\n","[Epoch: 4,   768/ 1068 points] loss for batch: 0.564\n","[Epoch: 4,   864/ 1068 points] loss for batch: 0.557\n","[Epoch: 4,   960/ 1068 points] loss for batch: 0.555\n","[Epoch: 4,  1056/ 1068 points] loss for batch: 0.551\n","Epoch finished, took 35.66 seconds.\n","Cumulated loss for epoch: 1.591468\n","***** Running evaluation *****\n","  Num examples = %d 356\n","  Batch size = %d 32\n","***** Eval results *****\n","   Loss: 1.560008\n","   Accuracy: 0.424157\n","   Precision (macro-averaged): 0.349421\n","   Recall (macro-averaged): 0.364539\n","   F1-Score (macro-averaged): 0.309511\n","time: 2min 55s\n"],"name":"stdout"}]}]}