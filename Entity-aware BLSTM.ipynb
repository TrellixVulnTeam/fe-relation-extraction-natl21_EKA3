{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Entity-aware BLSTM.ipynb","provenance":[{"file_id":"1Bson7svLNtsVwyda9M8bs-vIsX7Ilw3z","timestamp":1595416258343},{"file_id":"17yGp5vjcTs0lEV6ecS7bqXXSa2AjcEUq","timestamp":1591772554624},{"file_id":"1_i6d5uaAiqpuUwvf5NiHxcXHf58kAqt1","timestamp":1591254410656}],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm","mount_file_id":"1H41JbSX5zk3vjB6a57v_zlLG-QxzfWig","authorship_tag":"ABX9TyMO6pZ7Og/cRsFfhbj0StNI"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"80VEINjUPUm_"},"source":["# Entity aware Relation Classification"]},{"cell_type":"code","metadata":{"id":"LyM1i3r4HeDG"},"source":["%tensorflow_version 1.x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8H4Bw9tmPT7F"},"source":["import os\n","import sys\n","import time\n","import json\n","from pathlib import Path\n","import numpy as np\n","import pandas as pd\n","import datetime\n","import nltk\n","nltk.download('punkt')\n","import re\n","import tensorflow as tf\n","from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n","\n","if 'google.colab' in str(get_ipython()):\n","  print('Running on Google Colab')\n","  root = '/content/drive/My Drive/Colab Notebooks/'\n","else:\n","  print('Running locally')\n","  root = Path(os.getcwd()).parent\n","\n","basepath = os.path.join(root, 'relation-extraction/')\n","sys.path.append(os.path.join(basepath, 'entity-aware-relation-classification/code'))\n","\n","from model.entity_att_lstm import EntityAttentionLSTM\n","import utils\n","\n","import warnings\n","import sklearn.exceptions\n","warnings.filterwarnings(\"ignore\", category=sklearn.exceptions.UndefinedMetricWarning)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RLpt4eIeEb4C"},"source":["Switch for data usage: If True FewRel data will be used, if False Future Engineering data is used"]},{"cell_type":"code","metadata":{"id":"Uny2n32SEYDv"},"source":["use_fewrel_data=False"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0wZEfcLJUdv4"},"source":["max_sent_length = 90\n","\n","if (use_fewrel_data):\n","    data_dir = os.path.join(root, 'fewrel-training-data')\n","    run_dir = 'runs_fewrel'\n","\n","    train_path_fewrel = os.path.join(data_dir, 'fewrel/dev_7_classes_disjoint.json')\n","    test_path_fewrel = os.path.join(data_dir, 'fewrel/test_7_classes_disjoint.json')\n","    val_path_fewrel = os.path.join(data_dir, 'fewrel/train_7_classes_disjoint.json')\n","\n","    class2label = {'P105':0, 'P135':1, 'P155':2, 'P31':3, 'P800':4, 'P921':5, 'NOTA':6}\n","else:\n","    data_dir = os.path.join(root, 'fe-training-data')\n","    run_dir = 'runs_fe'\n","\n","    train_path_fewrel = os.path.join(data_dir, 'train_examples_nota_manufact_operate_operatesth_order_uses_ordersth.json')\n","    test_path_fewrel = os.path.join(data_dir, 'test_examples_nota_manufact_operate_operatesth_order_uses_ordersth.json')\n","    val_path_fewrel = os.path.join(data_dir, 'val_examples_nota_manufact_operate_operatesth_order_uses_ordersth.json')\n","\n","    class2label = {'NOTA':0, 'A manufactures product B':1, 'A operates B':2, 'A operates \\[something\\] in location B':3, 'A orders B':4, 'A uses/employs charging technology B':5, 'A orders something from B':6}\n","\n","\n","root_path = os.path.join(basepath, 'entity-aware-relation-classification')\n","\n","allow_soft_placement = True\n","log_device_placement = False\n","gpu_allow_growth = True\n","embedding_size = 300\n","pos_embedding_size = 50\n","hidden_size = 300\n","num_heads = 4\n","attention_size = 50\n","embeddings = \"glove300\"\n","l2_reg_lambda = 1e-5\n","\n","learning_rate = 1.0\n","decay_rate = 0.9\n","num_checkpoints = 1\n","batch_size = 20\n","num_epochs = 100\n","\n","emb_dropout_keep_prob = 0.7\n","rnn_dropout_keep_prob = 0.7\n","dropout_keep_prob = 0.5\n","\n","display_every = 10\n","evaluate_every = 100"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6wmjGyCTY73Z"},"source":["def logging_train(step, loss, accuracy):\n","    time_str = datetime.datetime.now().isoformat()\n","    print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n","\n","def logging_eval(step, loss, accuracy, predictions, labels):\n","    time_str = datetime.datetime.now().isoformat()\n","    print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n","\n","    accuracy = accuracy_score(predictions, labels)\n","    precision = precision_score(labels, predictions, average='macro', labels=np.unique(labels))\n","    recall = recall_score(labels, predictions, average='macro', labels=np.unique(labels))\n","    f1 = f1_score(labels, predictions, average='macro', labels=np.unique(labels))\n","\n","    print(\"{}: accuracy {:g}, precision {:g}, recall {:g}, f1 {:G}\\n\".format(time_str, accuracy, precision, recall, f1))\n","\n","    return f1, accuracy, precision, recall"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IdWRsyqpUIg5"},"source":["## Data Helpers"]},{"cell_type":"code","metadata":{"id":"kn-z9MrDUPFa"},"source":["def clean_str(text):\n","    text = text.lower()\n","    # Clean the text\n","    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n","    text = re.sub(r\"what's\", \"what is \", text)\n","    text = re.sub(r\"that's\", \"that is \", text)\n","    text = re.sub(r\"there's\", \"there is \", text)\n","    text = re.sub(r\"it's\", \"it is \", text)\n","    text = re.sub(r\"\\'s\", \" \", text)\n","    text = re.sub(r\"\\'ve\", \" have \", text)\n","    text = re.sub(r\"can't\", \"can not \", text)\n","    text = re.sub(r\"n't\", \" not \", text)\n","    text = re.sub(r\"i'm\", \"i am \", text)\n","    text = re.sub(r\"\\'re\", \" are \", text)\n","    text = re.sub(r\"\\'d\", \" would \", text)\n","    text = re.sub(r\"\\'ll\", \" will \", text)\n","    text = re.sub(r\",\", \" \", text)\n","    text = re.sub(r\"\\.\", \" \", text)\n","    text = re.sub(r\"!\", \" ! \", text)\n","    text = re.sub(r\"\\/\", \" \", text)\n","    text = re.sub(r\"\\^\", \" ^ \", text)\n","    text = re.sub(r\"\\+\", \" + \", text)\n","    text = re.sub(r\"\\-\", \" - \", text)\n","    text = re.sub(r\"\\=\", \" = \", text)\n","    text = re.sub(r\"'\", \" \", text)\n","    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n","    text = re.sub(r\":\", \" : \", text)\n","    text = re.sub(r\" e g \", \" eg \", text)\n","    text = re.sub(r\" b g \", \" bg \", text)\n","    text = re.sub(r\" u s \", \" american \", text)\n","    text = re.sub(r\"\\0s\", \"0\", text)\n","    text = re.sub(r\" 9 11 \", \"911\", text)\n","    text = re.sub(r\"e - mail\", \"email\", text)\n","    text = re.sub(r\"j k\", \"jk\", text)\n","    text = re.sub(r\"\\s{2,}\", \" \", text)\n","\n","    return text.strip()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aL0S5Anga1kc"},"source":["def load_data_and_labels_fewrel(path):\n","    with open(path, \"r\", encoding='utf-8') as f:\n","        lines = json.loads(f.read())\n","    \n","    data = []\n","    max_sentence_length = 0\n","\n","    for (i, line) in enumerate(lines):\n","        id = i\n","        for x in line['ents']:\n","            if x[1] == 1:\n","                x[1] = 0\n","        ents = line['ents']\n","        sentence = line['text']\n","        relation = line['label']\n","\n","        h = ents[0]\n","        t = ents[1]\n","        h_name = sentence[h[1]:h[2]]\n","        t_name = sentence[t[1]:t[2]]\n","        if h[1] < t[1]:\n","            sentence = sentence[:h[1]] + \" _e11_ \"+h_name+\" _e12_ \" + sentence[h[2]:t[1]] + \" _e21_ \"+t_name+\" _e22_ \" + sentence[t[2]:]\n","            \n","        else:\n","            sentence = sentence[:t[1]] + \" _e21_ \"+t_name+\" _e22_ \" + sentence[t[2]:h[1]] + \" _e11_ \"+h_name+\" _e12_ \" + sentence[h[2]:]\n","\n","        sentence = clean_str(sentence)\n","        tokens = nltk.word_tokenize(sentence)\n","        if max_sentence_length < len(tokens):\n","            max_sentence_length = len(tokens)\n","        e1 = tokens.index(\"e12\") - 1\n","        e2 = tokens.index(\"e22\") - 1\n","        sentence = \" \".join(tokens)\n","\n","        data.append([id, sentence, e1, e2, relation])\n","\n","    print(path)\n","    print(\"max sentence length = {}\\n\".format(max_sentence_length))\n","\n","    df = pd.DataFrame(data=data, columns=[\"id\", \"sentence\", \"e1\", \"e2\", \"relation\"])\n","\n","    pos1, pos2 = get_relative_position(df, max_sentence_length)\n","\n","    df['label'] = [class2label[r] for r in df['relation']]\n","\n","    # Text Data\n","    x_text = df['sentence'].tolist()\n","    e1 = df['e1'].tolist()\n","    e2 = df['e2'].tolist()\n","\n","    # Label Data\n","    y = df['label']\n","    labels_flat = y.values.ravel()\n","    labels_count = np.unique(labels_flat).shape[0]\n","\n","    # convert class labels from scalars to one-hot vectors\n","    # 0  => [1 0 0 0 0 ... 0 0 0 0 0]\n","    # 1  => [0 1 0 0 0 ... 0 0 0 0 0]\n","    # ...\n","    # 18 => [0 0 0 0 0 ... 0 0 0 0 1]\n","    def dense_to_one_hot(labels_dense, num_classes):\n","        num_labels = labels_dense.shape[0]\n","        index_offset = np.arange(num_labels) * num_classes\n","        labels_one_hot = np.zeros((num_labels, num_classes))\n","        labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n","        return labels_one_hot\n","\n","    labels = dense_to_one_hot(labels_flat, labels_count)\n","    labels = labels.astype(np.uint8)\n","\n","    return x_text, labels, e1, e2, pos1, pos2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EEIVJ0o1UPY7"},"source":["def get_relative_position(df, max_sentence_length):\n","    # Position data\n","    pos1 = []\n","    pos2 = []\n","    for df_idx in range(len(df)):\n","        sentence = df.iloc[df_idx]['sentence']\n","        tokens = nltk.word_tokenize(sentence)\n","        e1 = df.iloc[df_idx]['e1']\n","        e2 = df.iloc[df_idx]['e2']\n","\n","        p1 = \"\"\n","        p2 = \"\"\n","        for word_idx in range(len(tokens)):\n","            p1 += str((max_sentence_length - 1) + word_idx - e1) + \" \"\n","            p2 += str((max_sentence_length - 1) + word_idx - e2) + \" \"\n","        pos1.append(p1)\n","        pos2.append(p2)\n","\n","    return pos1, pos2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tvDrB_cOTrpP"},"source":["def batch_iter(data, batch_size, num_epochs, shuffle=True):\n","    \"\"\"\n","    Generates a batch iterator for a dataset.\n","    \"\"\"\n","    data = np.array(data)\n","    data_size = len(data)\n","    num_batches_per_epoch = int((len(data) - 1) / batch_size) + 1\n","    for epoch in range(num_epochs):\n","        # Shuffle the data at each epoch\n","        if shuffle:\n","            shuffle_indices = np.random.permutation(np.arange(data_size))\n","            shuffled_data = data[shuffle_indices]\n","        else:\n","            shuffled_data = data\n","        for batch_num in range(num_batches_per_epoch):\n","            start_index = batch_num * batch_size\n","            end_index = min((batch_num + 1) * batch_size, data_size)\n","            yield shuffled_data[start_index:end_index]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5U2GybHwU27j"},"source":["## Training"]},{"cell_type":"markdown","metadata":{"id":"1uLNW--cw3la"},"source":["Loading training-, test- and validation-datasets"]},{"cell_type":"code","metadata":{"id":"sHcgFKgWX04f"},"source":["with tf.device('/cpu:0'):\n","    train_text, train_y, train_e1, train_e2, train_pos1, train_pos2 = load_data_and_labels_fewrel(train_path_fewrel)\n","with tf.device('/cpu:0'):\n","    test_text, test_y, test_e1, test_e2, test_pos1, test_pos2 = load_data_and_labels_fewrel(test_path_fewrel)\n","with tf.device('/cpu:0'):\n","    val_text, val_y, val_e1, val_e2, val_pos1, val_pos2 = load_data_and_labels_fewrel(val_path_fewrel)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0mznqtoew7u1"},"source":["Building vocabulary"]},{"cell_type":"code","metadata":{"id":"k08hIJMjXzpy"},"source":["# Example: x_text[3] = \"A misty <e1>ridge</e1> uprises from the <e2>surge</e2>.\"\n","# ['a misty ridge uprises from the surge <UNK> <UNK> ... <UNK>']\n","# =>\n","# [27 39 40 41 42  1 43  0  0 ... 0]\n","# dimension = MAX_SENT_LENGTH\n","vocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor(max_sent_length)\n","vocab_processor.fit(train_text + test_text + val_text)\n","train_x = np.array(list(vocab_processor.transform(train_text)))\n","test_x = np.array(list(vocab_processor.transform(test_text)))\n","val_x = np.array(list(vocab_processor.transform(val_text)))\n","train_text = np.array(train_text)\n","test_text = np.array(test_text)\n","val_text = np.array(val_text)\n","\n","print(\"\\nText Vocabulary Size: {:d}\".format(len(vocab_processor.vocabulary_)))\n","print(\"train_x = {0}\".format(train_x.shape))\n","print(\"train_y = {0}\".format(train_y.shape))\n","print(\"test_x = {0}\".format(test_x.shape))\n","print(\"test_y = {0}\".format(test_y.shape))\n","print(\"val_x = {0}\".format(val_x.shape))\n","print(\"val_y = {0}\".format(val_y.shape))\n","\n","# Example: pos1[3] = [-2 -1  0  1  2   3   4 999 999 999 ... 999]\n","# [95 96 97 98 99 100 101 999 999 999 ... 999]\n","# =>\n","# [11 12 13 14 15  16  21  17  17  17 ...  17]\n","# dimension = MAX_SENT_LENGTH\n","pos_vocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor(max_sent_length)\n","pos_vocab_processor.fit(train_pos1 + train_pos2 + test_pos1 + test_pos2)\n","train_p1 = np.array(list(pos_vocab_processor.transform(train_pos1)))\n","train_p2 = np.array(list(pos_vocab_processor.transform(train_pos2)))\n","test_p1 = np.array(list(pos_vocab_processor.transform(test_pos1)))\n","test_p2 = np.array(list(pos_vocab_processor.transform(test_pos2)))\n","val_p1 = np.array(list(pos_vocab_processor.transform(val_pos1)))\n","val_p2 = np.array(list(pos_vocab_processor.transform(val_pos2)))\n","\n","print(\"\\nPosition Vocabulary Size: {:d}\".format(len(pos_vocab_processor.vocabulary_)))\n","print(\"train_p1 = {0}\".format(train_p1.shape))\n","print(\"test_p1 = {0}\".format(test_p1.shape))\n","print(\"val_p1 = {0}\".format(val_p1.shape))\n","print(\"\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gj-1mfYUwyCY"},"source":["Training process with TensorFlow"]},{"cell_type":"code","metadata":{"id":"M6WXVx-fU3Ix"},"source":["with tf.Graph().as_default():\n","    session_conf = tf.ConfigProto(\n","        allow_soft_placement=allow_soft_placement,\n","        log_device_placement=log_device_placement)\n","    session_conf.gpu_options.allow_growth = gpu_allow_growth\n","    sess = tf.Session(config=session_conf)\n","    with sess.as_default():\n","        model = EntityAttentionLSTM(\n","            sequence_length=train_x.shape[1],\n","            num_classes=train_y.shape[1],\n","            vocab_size=len(vocab_processor.vocabulary_),\n","            embedding_size=embedding_size,\n","            pos_vocab_size=len(pos_vocab_processor.vocabulary_),\n","            pos_embedding_size=pos_embedding_size,\n","            hidden_size=hidden_size,\n","            num_heads=num_heads,\n","            attention_size=attention_size,\n","            use_elmo=(embeddings == 'elmo'),\n","            l2_reg_lambda=l2_reg_lambda)\n","\n","        cur_best_f1 = 0.0\n","\n","        # Define Training procedure\n","        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n","        optimizer = tf.train.AdadeltaOptimizer(learning_rate, decay_rate, 1e-6)\n","        gvs = optimizer.compute_gradients(model.loss)\n","        capped_gvs = [(tf.clip_by_value(grad, -1.0, 1.0), var) for grad, var in gvs]\n","        train_op = optimizer.apply_gradients(capped_gvs, global_step=global_step)\n","\n","        # Output directory for models and summaries\n","        timestamp = str(int(time.time()))\n","        out_dir = os.path.abspath(os.path.join(root_path, run_dir, timestamp))\n","        print(\"\\nWriting to {}\\n\".format(out_dir))\n","\n","        # Summaries for loss and accuracy\n","        loss_summary = tf.summary.scalar(\"loss\", model.loss)\n","        acc_summary = tf.summary.scalar(\"accuracy\", model.accuracy)\n","\n","        # Train Summaries\n","        train_summary_op = tf.summary.merge([loss_summary, acc_summary])\n","        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n","        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n","\n","        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n","        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n","        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n","        if not os.path.exists(checkpoint_dir):\n","            os.makedirs(checkpoint_dir)\n","        saver = tf.train.Saver(tf.global_variables(), max_to_keep=num_checkpoints)\n","\n","        # Write vocabulary\n","        vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n","        pos_vocab_processor.save(os.path.join(out_dir, \"pos_vocab\"))\n","\n","        # Initialize all variables\n","        sess.run(tf.global_variables_initializer())\n","\n","        if embeddings == \"word2vec\":\n","            pretrain_W = utils.load_word2vec(os.path.join(root_path, 'resource/GoogleNews-vectors-negative300.bin'), embedding_size, vocab_processor)\n","            sess.run(model.W_text.assign(pretrain_W))\n","            print(\"Success to load pre-trained word2vec model!\\n\")\n","        elif embeddings == \"glove100\":\n","            pretrain_W = utils.load_glove(os.path.join(root_path, 'resource/glove.6B.100d.txt'), embedding_size, vocab_processor)\n","            sess.run(model.W_text.assign(pretrain_W))\n","            print(\"Success to load pre-trained glove100 model!\\n\")\n","        elif embeddings == \"glove300\":\n","            pretrain_W = utils.load_glove(os.path.join(root_path, 'resource/glove.840B.300d.txt'), embedding_size, vocab_processor)\n","            sess.run(model.W_text.assign(pretrain_W))\n","            print(\"Success to load pre-trained glove300 model!\\n\")\n","\n","        # Generate batches\n","        train_batches = batch_iter(list(zip(train_x, train_y, train_text,\n","                                                          train_e1, train_e2, train_p1, train_p2)),\n","                                                batch_size, num_epochs)\n","        # Training loop. For each batch...\n","        best_f1 = 0.0  # For save checkpoint(model)\n","        for train_batch in train_batches:\n","            train_bx, train_by, train_btxt, train_be1, train_be2, train_bp1, train_bp2 = zip(*train_batch)\n","            feed_dict = {\n","                model.input_x: train_bx,\n","                model.input_y: train_by,\n","                model.input_text: train_btxt,\n","                model.input_e1: train_be1,\n","                model.input_e2: train_be2,\n","                model.input_p1: train_bp1,\n","                model.input_p2: train_bp2,\n","                model.emb_dropout_keep_prob: emb_dropout_keep_prob,\n","                model.rnn_dropout_keep_prob: rnn_dropout_keep_prob,\n","                model.dropout_keep_prob: dropout_keep_prob\n","            }\n","            _, step, summaries, loss, accuracy = sess.run(\n","                [train_op, global_step, train_summary_op, model.loss, model.accuracy], feed_dict)\n","            train_summary_writer.add_summary(summaries, step)\n","\n","            # Training log display\n","            if step % display_every == 0:\n","                logging_train(step, loss, accuracy)\n","\n","            # Evaluation\n","            if step % evaluate_every == 0:\n","                print(\"\\nEvaluation:\")\n","                # Generate batches\n","                test_batches = batch_iter(list(zip(test_x, test_y, test_text,\n","                                                                test_e1, test_e2, test_p1, test_p2)),\n","                                                        batch_size, 1, shuffle=False)\n","                # Training loop. For each batch...\n","                losses = 0.0\n","                accuracy = 0.0\n","                predictions = []\n","                labels = []\n","                iter_cnt = 0\n","                for test_batch in test_batches:\n","                    test_bx, test_by, test_btxt, test_be1, test_be2, test_bp1, test_bp2 = zip(*test_batch)\n","\n","                    for elem in test_by:\n","                        labels.append(np.argmax(elem))\n","\n","                    feed_dict = {\n","                        model.input_x: test_bx,\n","                        model.input_y: test_by,\n","                        model.input_text: test_btxt,\n","                        model.input_e1: test_be1,\n","                        model.input_e2: test_be2,\n","                        model.input_p1: test_bp1,\n","                        model.input_p2: test_bp2,\n","                        model.emb_dropout_keep_prob: 1.0,\n","                        model.rnn_dropout_keep_prob: 1.0,\n","                        model.dropout_keep_prob: 1.0\n","                    }\n","                    loss, acc, pred = sess.run(\n","                        [model.loss, model.accuracy, model.predictions], feed_dict)\n","                    losses += loss\n","                    accuracy += acc\n","                    predictions += pred.tolist()\n","                    iter_cnt += 1\n","                losses /= iter_cnt\n","                accuracy /= iter_cnt\n","                predictions = np.array(predictions, dtype='int')\n","\n","                cur_f1, cur_accuracy, cur_precision, cur_recall = logging_eval(step, loss, accuracy, predictions, labels)\n","\n","                # Model checkpoint\n","                if best_f1 < cur_f1:\n","                    best_f1 = cur_f1\n","                    path = saver.save(sess, checkpoint_prefix+\"-{:.3g}\".format(best_f1), global_step=step)\n","                    print(\"Saved model checkpoint to {}\\n\".format(path))"],"execution_count":null,"outputs":[]}]}