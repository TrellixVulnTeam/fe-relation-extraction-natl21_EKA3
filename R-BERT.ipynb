{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"R-BERT.ipynb","provenance":[{"file_id":"1S-npM9Umn_s4D3FS6Fyl_pEo5fopcuob","timestamp":1593604374463},{"file_id":"1_i6d5uaAiqpuUwvf5NiHxcXHf58kAqt1","timestamp":1591254255357}],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm","mount_file_id":"1gY63ksihE7EJMYWiF7cV6dDN_xg6AMC3","authorship_tag":"ABX9TyNVqs3M3rmmKhGVl0qVpwf1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"jSZcpHBesDNw"},"source":["# **R-BERT Notebook**\n"]},{"cell_type":"markdown","metadata":{"id":"ndloW5ceTTDV"},"source":["## Imports and environment configuration"]},{"cell_type":"code","metadata":{"id":"vWyMo4eeshmP"},"source":["!pip install transformers==3.0.0\n","!pip install ipython-autotime\n","\n","%load_ext autotime"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1nDTaiQhzRbO"},"source":["import os\n","import sys\n","import json\n","import random\n","import numpy as np\n","from pathlib import Path\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","from torch.utils.data.distributed import DistributedSampler\n","from sklearn.metrics import f1_score, precision_score, recall_score\n","\n","random.seed(42)\n","\n","if 'google.colab' in str(get_ipython()):\n","  print('Running on Google Colab')\n","  root = '/content/drive/My Drive/Colab Notebooks/'\n","else:\n","  print('Running locally')\n","  root = Path(os.getcwd()).parent\n","\n","basepath = os.path.join(root, 'relation-extraction/')\n","sys.path.append(os.path.join(basepath, 'R-BERT/code'))\n","\n","from transformers import (BertConfig, BertTokenizer)\n","from transformers import AdamW, get_linear_schedule_with_warmup\n","\n","from model import BertForSequenceClassification\n","from utils import (convert_examples_to_features, InputExample)\n","\n","additional_special_tokens = [\"[E11]\", \"[E12]\", \"[E21]\", \"[E22]\"]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RLpt4eIeEb4C"},"source":["Switch for data usage: If True FewRel data will be used, if False Future Engineering data is used."]},{"cell_type":"code","metadata":{"id":"Uny2n32SEYDv"},"source":["use_fewrel_data=False"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G3CA6FHRJXbO"},"source":["if (use_fewrel_data):\n","    rel2id_map = {'P105':0, 'P135':1, 'P155':2, 'P31':3, 'P800':4, 'P921':5, 'NOTA':6}\n","    output_dir = os.path.join(basepath, 'R-BERT/output_fewrel')\n","    num_labels = 7\n","else:\n","    rel2id_map = {'NOTA':0, 'A manufactures product B':1, 'A operates B':2, 'A operates \\[something\\] in location B':3, 'A orders B':4, 'A uses/employs charging technology B':5, 'A orders something from B':6}\n","    output_dir = os.path.join(basepath, 'R-BERT/output_fe')\n","    num_labels = 7"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"avvs9sLFwa43"},"source":["Definition of fine-tuning parameters"]},{"cell_type":"code","metadata":{"id":"VwxaAdEqPI4W"},"source":["seed = 12345\n","pretrained_model_name='bert-base-uncased'\n","\n","num_train_epochs=5\n","learning_rate=3e-5\n","train_batch_size=16\n","eval_batch_size=8\n","no_cuda=False\n"," \n","max_seq_len=128\n","\n","evaluate_during_training=True\n","\n","gradient_accumulation_steps=1\n"," \n","weight_decay=1e-3\n","adam_epsilon=1e-8\n","max_grad_norm=1.0\n","\n","max_steps=-1\n","warmup_steps=0\n","logging_steps=40\n","\n","local_rank=-1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KQbydbS12POI"},"source":["if (use_fewrel_data): \n","    data_dir = os.path.join(root, 'fewrel-training-data/fewrel/')\n","    train_data_file = \"dev_%d_classes_disjoint.json\" % num_labels\n","    test_data_file = \"test_%d_classes_disjoint.json\" % num_labels\n","else:\n","    data_dir = os.path.join(root, 'fe-training-data')\n","    train_data_file = 'train_examples_nota_manufact_operate_operatesth_order_uses_ordersth.json'\n","    test_data_file = 'test_examples_nota_manufact_operate_operatesth_order_uses_ordersth.json'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i4ZJyGe0xerP"},"source":["FewRelProcessor class for extracting examples and input features from FewRel and Future Engineering input files."]},{"cell_type":"code","metadata":{"id":"wueJApWgsa0H"},"source":["class FewrelProcessor():\n","    def get_train_examples(self, data_dir,file_name):\n","        examples = self._create_examples(\n","            self._read_json(os.path.join(data_dir, file_name)), \"train\")\n","        labels = set([x.label for x in examples])\n","        return examples, list(labels)\n","\n","    def get_dev_examples(self, data_dir,file_name):\n","        examples = self._create_examples(\n","            self._read_json(os.path.join(data_dir, file_name)), \"dev\")\n","        labels = set([x.label for x in examples])\n","        return examples, list(labels)\n","        \n","    def get_test_examples(self, data_dir, file_name):\n","        examples = self._create_examples(\n","            self._read_json(os.path.join(data_dir, file_name)), \"test\")\n","        labels = set([x.label for x in examples])\n","        return examples, list(labels)\n","\n","    def _create_examples(self, lines, set_type):\n","        \"\"\"Creates examples for the training and dev sets.\"\"\"\n","        examples = []\n","        for (i, line) in enumerate(lines):\n","            guid = \"%s-%s\" % (set_type, i)\n","            for x in line['ents']:\n","                if x[1] == 1:\n","                    x[1] = 0\n","            text_a = (line['text'], line['ents'])\n","            ex_text_a = line['text']\n","            h = line['ents'][0]\n","            t = line['ents'][1]\n","            h_name = ex_text_a[h[1]:h[2]]\n","            t_name = ex_text_a[t[1]:t[2]]\n","            if h[1] < t[1]:\n","                ex_text_a = ex_text_a[:h[1]] + \" [E11] \"+h_name+\" [E12] \" + ex_text_a[h[2]:t[1]] + \" [E21] \"+t_name+\" [E22] \" + ex_text_a[t[2]:]\n","                \n","            else:\n","                ex_text_a = ex_text_a[:t[1]] + \" [E21] \"+t_name+\" [E22] \" + ex_text_a[t[2]:h[1]] + \" [E11] \"+h_name+\" [E12] \" + ex_text_a[h[2]:]\n","            text_a = ex_text_a\n","            label = rel2id_map[line['label']]\n","            examples.append(InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n","        return examples\n","\n","    def _read_json(cls, input_file):\n","        with open(input_file, \"r\", encoding='utf-8') as f:\n","            return json.loads(f.read())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vR2P4-MqxsZa"},"source":["Initialization of model and tokenizer"]},{"cell_type":"code","metadata":{"id":"Hn1afSK2QX6H"},"source":["if local_rank == -1 or no_cuda:\n","    device = torch.device(\n","        \"cuda\" if torch.cuda.is_available() and not no_cuda else \"cpu\")\n","    # config.n_gpu = torch.cuda.device_count()\n","    n_gpu = 1\n","else:\n","    # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n","    torch.cuda.set_device(local_rank)\n","    device = torch.device(\"cuda\", local_rank)\n","    torch.distributed.init_process_group(backend='nccl')\n","    n_gpu = 1\n","\n","bertconfig = BertConfig.from_pretrained(pretrained_model_name, num_labels=num_labels)\n","do_lower_case = \"-uncased\" in pretrained_model_name\n","\n","tokenizer = BertTokenizer.from_pretrained(pretrained_model_name, do_lower_case=do_lower_case, additional_special_tokens=additional_special_tokens)    \n","\n","model = BertForSequenceClassification.from_pretrained(pretrained_model_name, config=bertconfig)\n","model.resize_token_embeddings(len(tokenizer))\n","model.to(device)\n","\n","print('Loaded model & tokenizer')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4xorICoDxxom"},"source":["Helper function to calculate accuracy, precision, recall and f1 values"]},{"cell_type":"code","metadata":{"id":"mokgS7_lv3jH"},"source":["def accuracy_precision_recall_f1(outputs, labels):\n","    accuracy = np.sum(outputs == labels)/len(outputs)\n","    precision = precision_score(labels, outputs, average='macro', labels=np.unique(labels))\n","    recall = recall_score(labels, outputs, average='macro', labels=np.unique(labels))\n","    f1 = f1_score(labels, outputs, average='macro', labels=np.unique(labels))\n","    return accuracy, precision, recall, f1, outputs"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uZVdhm7yx3yn"},"source":["Helper function for evaluating while training process"]},{"cell_type":"code","metadata":{"id":"2H_2SCKjRp10"},"source":["def evaluate(model, tokenizer, eval_dataset, prefix=\"\"):\n","    results = {}\n","\n","    eval_sampler = SequentialSampler(eval_dataset) if local_rank == -1 else DistributedSampler(eval_dataset)\n","    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=eval_batch_size, shuffle=False)\n","\n","    print(\"***** Running evaluation *****\")\n","    print(\"  Num examples = %d\" % len(eval_dataset))\n","    print(\"  Batch size = %d\" % eval_batch_size)\n","    eval_loss = 0.0\n","    nb_eval_steps = 0\n","    preds = None\n","    out_label_ids = None\n","    for batch in eval_dataloader:\n","        model.eval()\n","        batch = tuple(t.to(device) for t in batch)\n","\n","        with torch.no_grad():\n","            inputs = {'input_ids':      batch[0],\n","                      'attention_mask': batch[1],\n","                      'token_type_ids': batch[2],\n","                      'labels':      batch[3],\n","                      'e1_mask': batch[4],\n","                      'e2_mask': batch[5],\n","                      }\n","            outputs = model(**inputs)\n","            tmp_eval_loss, logits = outputs[:2]\n","\n","            eval_loss += tmp_eval_loss.mean().item()\n","        nb_eval_steps += 1\n","        if preds is None:\n","            preds = logits.detach().cpu().numpy()\n","            out_label_ids = inputs['labels'].detach().cpu().numpy()\n","        else:\n","            preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n","            out_label_ids = np.append(\n","                out_label_ids, inputs['labels'].detach().cpu().numpy(), axis=0)\n","\n","    eval_loss = eval_loss / nb_eval_steps\n","    preds = np.argmax(preds, axis=1)\n","\n","    eval_accuracy, eval_precision, eval_recall, eval_f1,_ = accuracy_precision_recall_f1(preds, out_label_ids)\n","\n","    print(\"***** Eval results *****\")\n","    print(\"   Loss: %f\" % eval_loss)\n","    print(\"   Accuracy: %f\" % eval_accuracy)\n","    print(\"   Precision (macro-averaged): %f\" % eval_precision)\n","    print(\"   Recall (macro-averaged): %f\" % eval_recall)\n","    print(\"   F1-Score (macro-averaged): %f\" % eval_f1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r-_ZqSIWx9RE"},"source":["Helper function for loading and extracting input features from training data files"]},{"cell_type":"code","metadata":{"id":"rdNrGRXcwo15"},"source":["def load_and_cache_fewrel_examples(tokenizer, evaluate=False):\n","    processor = FewrelProcessor()\n","\n","    if (evaluate):\n","        train_examples, train_label_list = processor.get_train_examples(data_dir, test_data_file)\n","    else:\n","        train_examples, train_label_list = processor.get_dev_examples(data_dir, train_data_file)\n","    \n","    train_features = convert_examples_to_features(train_examples, train_label_list, max_seq_len, tokenizer, \"classification\", use_entity_indicator=True)\n","\n","    # Convert to Tensors and build dataset\n","    all_input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long)\n","    all_input_mask = torch.tensor([f.input_mask for f in train_features], dtype=torch.long)\n","    all_segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype=torch.long)\n","    all_e1_mask = torch.tensor([f.e1_mask for f in train_features], dtype=torch.long)  # add e1 mask\n","    all_e2_mask = torch.tensor([f.e2_mask for f in train_features], dtype=torch.long)  # add e2 mask\n","\n","    all_label_ids = torch.tensor([f.label_id for f in train_features], dtype=torch.long)\n","    \n","    dataset = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids, all_e1_mask, all_e2_mask)\n","    return dataset"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xB931FbjSLu_"},"source":["## Fine-Tuning"]},{"cell_type":"markdown","metadata":{"id":"ZpwxYeXuyECt"},"source":["Loading training and validation data"]},{"cell_type":"code","metadata":{"id":"PD7WOYYHTbEn"},"source":["train_dataset = load_and_cache_fewrel_examples(tokenizer)\n","eval_dataset = load_and_cache_fewrel_examples(tokenizer, evaluate=True)\n","\n","if local_rank == -1:\n","    train_sampler = RandomSampler(train_dataset)\n","else:\n","    DistributedSampler(train_dataset)\n","\n","train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=train_batch_size)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HHiCw7wXyGxu"},"source":["Preparing optimizer and scheduler for training process"]},{"cell_type":"code","metadata":{"id":"dcMXl4AaPB9Q"},"source":["if max_steps > 0:\n","    t_total = max_steps\n","    num_train_epochs = max_steps // (len(train_dataloader) // gradient_accumulation_steps) + 1\n","else:\n","    t_total = len(train_dataloader) // gradient_accumulation_steps * num_train_epochs\n","\n","no_decay = ['bias', 'LayerNorm.weight']\n","optimizer_grouped_parameters = [\n","    {'params': [p for n, p in model.named_parameters()\n","                if not any(nd in n for nd in no_decay)], 'weight_decay': weight_decay},\n","    {'params': [p for n, p in model.named_parameters()\n","                if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n","]\n","optimizer = AdamW(optimizer_grouped_parameters,lr=learning_rate, eps=adam_epsilon)\n","scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=t_total)\n","if n_gpu > 1:\n","    model = torch.nn.DataParallel(model)\n","\n","if local_rank != -1:\n","    model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[local_rank],\n","                                                      output_device=local_rank,\n","                                                      find_unused_parameters=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PwUz4-bHyLXy"},"source":["Training process of the approach"]},{"cell_type":"code","metadata":{"id":"nK3enke2TxGe","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1603354657186,"user_tz":-120,"elapsed":82465,"user":{"displayName":"Christoph Brandl","photoUrl":"","userId":"16180149632937868484"}},"outputId":"0e2b85d3-8194-4607-9edd-3a1b89ae356e"},"source":["print(\"***** Running training *****\")\n","print(\"  Num examples = %d\" % len(train_dataset))\n","print(\"  Num Epochs = %d\" % num_train_epochs)\n","print(\"  Train batch size = %d\" % train_batch_size)\n","print(\"  Gradient Accumulation steps = %d\" % gradient_accumulation_steps)\n","print(\"  Total optimization steps = %d\" % t_total)\n","\n","global_step = 0\n","tr_loss, logging_loss = 0.0, 0.0\n","model.zero_grad()\n","\n","for epoch in range(num_train_epochs):\n","    print(\"------------- Epoch %d -------------\" % (epoch+1))\n","    for step, batch in enumerate(train_dataloader):\n","        model.train()\n","        batch = tuple(t.to(device) for t in batch)\n","        inputs = {'input_ids':      batch[0],\n","                  'attention_mask': batch[1],\n","                  'token_type_ids': batch[2],\n","                  'labels':      batch[3],\n","                  'e1_mask': batch[4],\n","                  'e2_mask': batch[5],\n","                  }\n","\n","        outputs = model(**inputs)\n","        loss = outputs[0]\n","        if n_gpu > 1:\n","            loss = loss.mean()  # mean() to average on multi-gpu parallel training\n","        if gradient_accumulation_steps > 1:\n","            loss = loss / gradient_accumulation_steps\n","\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n","\n","        tr_loss += loss.item()\n","\n","        if (step + 1) % logging_steps == 0:\n","            print('[Epoch: %d, Step: %d] average loss: %.3f' % (epoch + 1, (step + 1), tr_loss/(step + (len(train_dataset)/train_batch_size)*epoch)))\n","\n","        if (step + 1) % gradient_accumulation_steps == 0:\n","            optimizer.step()\n","            scheduler.step()  # Update learning rate schedule\n","            model.zero_grad()\n","            global_step += 1        \n","\n","    if local_rank == -1 and evaluate_during_training:\n","        evaluate(model, tokenizer, eval_dataset)    \n","\n","print('-------------------')\n","print('Training results')\n","print(\" global_step = %s, average loss = %s\" % (global_step, tr_loss/global_step))\n","\n","model_to_save = model.module if hasattr(model, 'module') else model\n","model_to_save.save_pretrained(output_dir)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["***** Running training *****\n","  Num examples = 1068\n","  Num Epochs = 5\n","  Train batch size = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 335\n","------------- Epoch 1 -------------\n","[Epoch: 1, Step: 40] average loss: 1.570\n","***** Running evaluation *****\n","  Num examples = 356\n","  Batch size = 8\n","***** Eval results *****\n","   Loss: 0.815295\n","   Accuracy: 0.747191\n","   Precision (macro-averaged): 0.742199\n","   Recall (macro-averaged): 0.742868\n","   F1-Score (macro-averaged): 0.738781\n","------------- Epoch 2 -------------\n","[Epoch: 2, Step: 40] average loss: 1.017\n","***** Running evaluation *****\n","  Num examples = 356\n","  Batch size = 8\n","***** Eval results *****\n","   Loss: 0.653049\n","   Accuracy: 0.792135\n","   Precision (macro-averaged): 0.800121\n","   Recall (macro-averaged): 0.787660\n","   F1-Score (macro-averaged): 0.791628\n","------------- Epoch 3 -------------\n","[Epoch: 3, Step: 40] average loss: 0.754\n","***** Running evaluation *****\n","  Num examples = 356\n","  Batch size = 8\n","***** Eval results *****\n","   Loss: 0.704556\n","   Accuracy: 0.803371\n","   Precision (macro-averaged): 0.811820\n","   Recall (macro-averaged): 0.788808\n","   F1-Score (macro-averaged): 0.794057\n","------------- Epoch 4 -------------\n","[Epoch: 4, Step: 40] average loss: 0.593\n","***** Running evaluation *****\n","  Num examples = 356\n","  Batch size = 8\n","***** Eval results *****\n","   Loss: 0.668985\n","   Accuracy: 0.825843\n","   Precision (macro-averaged): 0.818362\n","   Recall (macro-averaged): 0.817446\n","   F1-Score (macro-averaged): 0.816358\n","------------- Epoch 5 -------------\n","[Epoch: 5, Step: 40] average loss: 0.487\n","***** Running evaluation *****\n","  Num examples = 356\n","  Batch size = 8\n","***** Eval results *****\n","   Loss: 0.680141\n","   Accuracy: 0.817416\n","   Precision (macro-averaged): 0.813478\n","   Recall (macro-averaged): 0.808210\n","   F1-Score (macro-averaged): 0.808398\n","-------------------\n","Training results\n"," global_step = 335, average loss = 0.4513571857627648\n","time: 1min 21s\n"],"name":"stdout"}]}]}