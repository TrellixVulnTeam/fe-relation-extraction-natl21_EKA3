{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Trainingdata Evaluation & Generation.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"mount_file_id":"1fk7sgRFlucszo1geP-TdRobmA59k69Tz","authorship_tag":"ABX9TyP+J2JpMPtZC2FTYXGv1MS3"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"qWEgmjGPGFf6"},"source":["# Data preparation & evaluation Notebook"]},{"cell_type":"markdown","metadata":{"id":"nLNffK-FroJR"},"source":["This notebook provides a collection of different functions for generation & evaluating training data from inception data as well as the selecting of training data from FewRel dataset. "]},{"cell_type":"code","metadata":{"id":"0vRC0P-lsypN"},"source":["import json\n","import os\n","import random\n","import copy\n","from nltk.tokenize import sent_tokenize\n","from pathlib import Path\n","\n","if 'google.colab' in str(get_ipython()):\n","  print('Running on Google Colab')\n","  root = '/content/drive/My Drive/Colab Notebooks/'\n","else:\n","  print('Running locally')\n","  root = Path(os.getcwd()).parent\n","\n","basepath = os.path.join(root, 'relation-extraction/')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h4XBa5GRGPPN"},"source":["## Preparation of annotated data generated with Inception "]},{"cell_type":"code","metadata":{"id":"-NnP1JMAza0y"},"source":["data_dir = \"fe-training-data/\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EfQT-Pw6GkdA"},"source":["Read JSON file with all extracted examples"]},{"cell_type":"code","metadata":{"id":"EgijcAOEtBuI"},"source":["inputfile = os.path.join(root, os.path.join(data_dir, \"all_examples.json\"))\n","with open(inputfile, \"r\") as in_file:\n","    lines = json.load(in_file)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"19KNEDrFBkgo"},"source":["outputfile = os.path.join(root, os.path.join(data_dir, \"examples_nota_manufact_operate_operatesth_order_uses_ordersth.json\"))\n","train_file = os.path.join(root, os.path.join(data_dir, \"train_examples_nota_manufact_operate_operatesth_order_uses_ordersth.json\"))\n","test_file = os.path.join(root, os.path.join(data_dir, \"test_examples_nota_manufact_operate_operatesth_order_uses_ordersth.json\"))\n","val_file = os.path.join(root, os.path.join(data_dir, \"val_examples_nota_manufact_operate_operatesth_order_uses_ordersth.json\"))\n","\n","relevant_relations = ['NOTA', 'A manufactures product B', 'A operates B', 'A operates \\[something\\] in location B', 'A orders B', 'A uses/employs charging technology B', 'A orders something from B']\n","relevant_relations_9 = ['NOTA', 'A manufactures product B', 'A operates B', 'A operates \\[something\\] in location B', 'A orders B', 'A uses/employs charging technology B', 'A orders something from B', 'A researches/develops technology or product B', 'A delivers something to B']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HfExdMOaGrW0"},"source":["### Data evaluation and insights"]},{"cell_type":"markdown","metadata":{"id":"Bc3NT5wos4cl"},"source":["Helper function for cleaning the incoming examples from inception data."]},{"cell_type":"code","metadata":{"id":"8fLvOMIgGUlB"},"source":["def clean_examples(data):\n","    examples = copy.deepcopy(data)\n","    things_to_clean = ['(', '-', ',']\n","    cleaned_examples = []\n","    for sent in examples:\n","        if (not sent['ents'][0][0].isnumeric() and not sent['ents'][1][0].isnumeric()):\n","            for thing in things_to_clean:\n","                # check for trailing odd character and remove it\n","                if (sent['ents'][0][0].endswith(thing)):\n","                    sent['ents'][0][0] = sent['ents'][0][0][:-1]\n","                    sent['ents'][0][2] -= 1\n","                    # check for trailing whitespace and remove it\n","                    if (sent['ents'][0][0].endswith(' ')):\n","                        sent['ents'][0][0] = sent['ents'][0][0][:-1]\n","                        sent['ents'][0][2] -= 1\n","                # do the same for second entity\n","                if (sent['ents'][1][0].endswith(thing)):\n","                    sent['ents'][1][0] = sent['ents'][1][0][:-1]\n","                    sent['ents'][1][2] -= 1\n","                    if (sent['ents'][1][0].endswith(' ')):\n","                        sent['ents'][1][0] = sent['ents'][1][0][:-1]\n","                        sent['ents'][1][2] -= 1\n","\n","            if (not sent['ents'][0][0].isnumeric() and not sent['ents'][1][0].isnumeric() and sent['ents'][0][0] and sent['ents'][1][0]):\n","                cleaned_examples.append(sent)\n","            else:\n","                print(sent)\n","    return cleaned_examples"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RPqctcvus_Ut"},"source":["Counting the number of examples per relation and the distinct number of sentence sequences."]},{"cell_type":"code","metadata":{"id":"grf2FaXJQP-M","colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"status":"ok","timestamp":1603890063262,"user_tz":-60,"elapsed":1472,"user":{"displayName":"Christoph Brandl","photoUrl":"","userId":"16180149632937868484"}},"outputId":"65361102-1a63-4576-de14-5310d9a0d066"},"source":["valid_training_data = clean_examples([x for x in lines if x['label'] in relevant_relations])\n","\n","print('Number of relevant examples in dataset: %d' % len(valid_training_data))\n","print('Number of distinct origin sentences: %d' % len(set([x['text'] for x in valid_training_data])))\n","\n","valid_exp_per_label = dict()\n","for line in valid_training_data:\n","    if (line['label'] not in valid_exp_per_label.keys()):\n","        valid_exp_per_label[line['label']] = []\n","    valid_exp_per_label[line['label']].append(line)\n","\n","print('\\nOverview of examples per relation type:')\n","valid_x = {k: len(v) for k, v in valid_exp_per_label.items()}\n","for key in {k: v for k, v in sorted(valid_x.items(), key=lambda item: item[1], reverse=True)}:\n","    print('%d examples for relation %s' %(len(valid_exp_per_label[key]), key))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Number of relevant examples in dataset: 1780\n","Number of distinct origin sentences: 707\n","\n","Overview of examples per relation type:\n","396 examples for relation A manufactures product B\n","345 examples for relation A orders B\n","286 examples for relation NOTA\n","236 examples for relation A operates B\n","200 examples for relation A operates \\[something\\] in location B\n","160 examples for relation A uses/employs charging technology B\n","157 examples for relation A orders something from B\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"w07f-NULtQq5"},"source":["Dumping the extracted valid examples to an outputfile."]},{"cell_type":"code","metadata":{"id":"IzEw17n8MJBX"},"source":["with open(outputfile, \"w\") as out_file:\n","    json.dump(valid_training_data, out_file)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Or-bvgM4Gx7Z"},"source":["### Train-Test-Split of data"]},{"cell_type":"markdown","metadata":{"id":"YXgcvX3hsLI2"},"source":["Splitting the generated examples into disjoint training-, test- and validation-datasets"]},{"cell_type":"code","metadata":{"id":"OtdhlUjOyDYn"},"source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","\n","df = pd.read_json(open(outputfile, \"r\"))\n","y = df[['label']]\n","\n","train, test, y_train, _ = train_test_split(df, y, test_size=0.2, stratify=y)\n","train, val, _, _ = train_test_split(train, y_train, test_size=0.25, stratify=y_train)\n","\n","train_examples = json.loads(train.to_json(orient=\"records\"))\n","test_examples = json.loads(test.to_json(orient=\"records\"))\n","val_examples = json.loads(val.to_json(orient=\"records\"))\n","\n","with open(train_file, \"w\") as out_file:\n","    json.dump(train_examples, out_file)\n","with open(test_file, \"w\") as out_file:\n","    json.dump(test_examples, out_file)\n","with open(val_file, \"w\") as out_file:\n","    json.dump(val_examples, out_file)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"52IUku5v3YdN","colab":{"base_uri":"https://localhost:8080/","height":68},"executionInfo":{"status":"ok","timestamp":1603353197302,"user_tz":-120,"elapsed":1682,"user":{"displayName":"Christoph Brandl","photoUrl":"","userId":"16180149632937868484"}},"outputId":"e7c69486-5291-4905-a986-bc883bfaf6dd"},"source":["print('Training dataset length: %d' % len(train))\n","print('Test dataset length: %d' % len(test))\n","print('Validation dataset length: %d' % len(val))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Training dataset length: 1068\n","Test dataset length: 356\n","Validation dataset length: 356\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"lDz9utjoG5Zj"},"source":["### Generation of Per-label-format (needed for BERT Pair)"]},{"cell_type":"markdown","metadata":{"id":"rN9DY5Odsbei"},"source":["Helper function for converting a sentence with entities to the needed format of BERT Pair with the entity containing tokens instead of fixed positions in the sentence."]},{"cell_type":"code","metadata":{"id":"mAJCesOBQLgc"},"source":["def convert_sentence_to_query(sentence, ent1, ent2):\n","    ent1_name = sentence[ent1[1]-1:ent1[2]]\n","    ent1_start = ent1[1]\n","    ent1_end = ent1[2]\n","    ent2_name = sentence[ent2[1]-1:ent2[2]]\n","    ent2_start = ent2[1]\n","    ent2_end = ent2[2]\n","    \n","    tokens = sentence.split()\n","\n","    ent1_tokens = []\n","    ent2_tokens = []\n","    pos = 0\n","\n","    for i, token in enumerate(tokens):\n","        if ((pos >= ent1_start-1 and pos <= ent1_end) or (pos >= ent1_start-2 and pos <= ent1_end+2 and ent1_name in token)):\n","            ent1_tokens.append(i)\n","        if ((pos >= ent2_start-1 and pos <= ent2_end) or (pos >= ent2_start-2 and pos <= ent2_end+2 and ent2_name in token)):\n","            ent2_tokens.append(i)\n","        \n","        pos += len(token)+1\n","\n","    sentence_obj = dict()\n","    sentence_obj['tokens'] = tokens\n","    sentence_obj['h'] = [ent1_name.lower().lstrip(), '', [ent1_tokens]]\n","    sentence_obj['t'] = [ent2_name.lower().lstrip(), '', [ent2_tokens]]\n","\n","    if (len(ent1_tokens) == 0 or len(ent2_tokens) == 0):\n","        print (sentence_obj)\n","        \n","    return sentence_obj"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cp0Du2XEsrM-"},"source":["Generation of the BERT Pair specific format for train-, test- and validation-dataset"]},{"cell_type":"code","metadata":{"id":"dOVMPFmYLrUJ"},"source":["data_files = [train_file, test_file, val_file]\n","for data_file in data_files:\n","    with open(data_file, \"r\") as in_file:\n","        lines = json.load(in_file)\n","\n","    exp_per_label = dict()\n","    for line in lines:\n","        if (line['label'] != 'NOTA'):\n","            if (line['label'] not in exp_per_label.keys()):\n","                exp_per_label[line['label']] = []\n","            exp_per_label[line['label']].append(line)\n","    \n","    output_examples = dict()\n","    for key in exp_per_label:\n","        output_examples[key] = []\n","        for example in exp_per_label[key]:\n","            output_example = convert_sentence_to_query(example['text'], example['ents'][0], example['ents'][1])\n","            output_examples[key].append(output_example)\n","\n","    with open(os.path.splitext(data_file)[0] + \"_per_label.json\", \"w\") as out_file:\n","        json.dump(output_examples, out_file)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vt1R-IAmJbRN"},"source":["## Generation of Test and Train data from FewRel containing NOTA class"]},{"cell_type":"markdown","metadata":{"id":"tYOzlXrXNFhn"},"source":["Definition of file names and relevant relation classes"]},{"cell_type":"code","metadata":{"id":"HbZE9mNgJue5"},"source":["data_dir = os.path.join(root, 'fewrel-training-data/fewrel/')\n","\n","in_train_data_file = os.path.join(data_dir,\"train_80_classes.json\")\n","in_test_data_file = os.path.join(data_dir,\"test_80_classes.json\")\n","in_dev_data_file = os.path.join(data_dir,\"dev_80_classes.json\")\n","\n","out_train_data_file = os.path.join(data_dir,\"train_7_classes_disjoint.json\")\n","out_test_data_file = os.path.join(data_dir,\"test_7_classes_disjoint.json\")\n","out_dev_data_file = os.path.join(data_dir,\"dev_7_classes_disjoint.json\")\n","\n","relevant_relations = ['P105', 'P135', 'P155', 'P31', 'P800', 'P921']\n","\n","nota_train_classes = ['P740', 'P26', 'P710', 'P86', 'P931', 'P361', 'P3450', 'P57', 'P6', 'P175', 'P22', 'P1877', 'P1411', 'P178', 'P127', 'P1923', 'P412', 'P2094', 'P1408', 'P137', 'P39', 'P974', 'P118', 'P136', 'P27', 'P364', 'P150', 'P40', 'P176', 'P1303', 'P495', 'P1346', 'P937', 'P25', 'P264', 'P102', 'P460', 'P159', 'P400', 'P991']\n","\n","nota_inference_classes = random.sample(['P407', 'P403', 'P449', 'P551', 'P59', 'P177', 'P674', 'P706', 'P527', 'P84', 'P306', 'P123', 'P750', 'P413', 'P206', 'P466', 'P131', 'P463', 'P3373', 'P58', 'P1344', 'P1435', 'P410', 'P140', 'P355', 'P101', 'P156', 'P241', 'P641', 'P276', 'P4552', 'P1001', 'P106', 'P17'],25)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kOB5WAf0uHIQ"},"source":["Reading FewRel data from inputfile"]},{"cell_type":"code","metadata":{"id":"Mpcfo4fBVrXE"},"source":["with open(in_train_data_file, 'r', encoding='utf-8') as inputfile:\n","    data = json.load(inputfile)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OT4d26deuMCv"},"source":["Helper function for generating a random dataset of given input examples with artificially created NOTA class"]},{"cell_type":"code","metadata":{"id":"Xr53HZuyq64H"},"source":["def generate_dataset_with_nota(input_data_file, relevant_relations, nota_classes):\n","    def set_label_to_nota(elem):\n","        elem['label'] = 'NOTA'\n","        return elem\n","\n","    with open(input_data_file, 'r', encoding='utf-8') as inputfile:\n","        data = json.load(inputfile)\n","        \n","        relevant_data = [x for x in data if x['label'] in relevant_relations]\n","\n","        examples_per_class = int(int(len(relevant_data)/len(relevant_relations))/len(nota_classes))\n","        \n","        nota_data_per_rel = dict()\n","        nota_data = [x for x in data if x['label'] in nota_classes]\n","\n","        for example in nota_data:\n","            if (example['label'] not in nota_data_per_rel):\n","                nota_data_per_rel[example['label']] = []\n","            nota_data_per_rel[example['label']].append(example)\n","\n","        for relation in nota_data_per_rel:\n","            random_examples = random.sample(nota_data_per_rel[relation], examples_per_class)\n","            random_examples = list(map(set_label_to_nota, random_examples))\n","            relevant_data.extend(random_examples)\n","        \n","        print('Length of generated dataset: %d' % len(relevant_data))\n","    return relevant_data"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NgdKmcLjtu1Y"},"source":["Generating and saving train-, test- and validation-dataset from FewRel data containing NOTA-examples as well."]},{"cell_type":"code","metadata":{"id":"222ox53UJ1Gr","colab":{"base_uri":"https://localhost:8080/","height":68},"executionInfo":{"status":"ok","timestamp":1600341634568,"user_tz":-120,"elapsed":1005,"user":{"displayName":"Christoph Brandl","photoUrl":"","userId":"16180149632937868484"}},"outputId":"32783fdd-6754-405b-b305-3eadc9cc2531"},"source":["with open(out_train_data_file, 'w') as outputfile:\n","    train_data = generate_dataset_with_nota(in_train_data_file, relevant_relations, nota_inference_classes)\n","    json.dump(train_data, outputfile)\n","\n","with open(out_test_data_file, 'w') as outputfile:\n","    test_data = generate_dataset_with_nota(in_test_data_file, relevant_relations, nota_inference_classes)\n","    json.dump(test_data, outputfile)\n","\n","with open(out_dev_data_file, 'w') as outputfile:\n","    dev_data = generate_dataset_with_nota(in_dev_data_file, relevant_relations, nota_train_classes)\n","    json.dump(dev_data, outputfile)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Length of generated dataset: 700\n","Length of generated dataset: 1400\n","Length of generated dataset: 1400\n"],"name":"stdout"}]}]}